<?xml version="1.0" encoding="utf-8"?><?xml-stylesheet type="text/xml" href="http://localhost:4000/feed.xslt.xml"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="http://jekyllrb.com" version="3.3.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2016-11-28T22:36:54+02:00</updated><id>http://localhost:4000//</id><title type="html">The Metal Framework</title><subtitle>Resources and tutorials for Metal, MetalKit and Metal Performance Shaders.
</subtitle><entry><title type="html">Using MetalKit part 2*3^2</title><link href="http://localhost:4000/2016/10/01/using-metalkit-part-2-3-2.html" rel="alternate" type="text/html" title="Using MetalKit part 2*3^2" /><published>2016-10-01T00:00:00+03:00</published><updated>2016-10-01T00:00:00+03:00</updated><id>http://localhost:4000/2016/10/01/using-metalkit-part-2-3-2</id><content type="html" xml:base="http://localhost:4000/2016/10/01/using-metalkit-part-2-3-2.html">Yes, as the title suggests, we're going to have another of those posts with math (and fun) in it. I was thinking the other day, what can we do while commuting for an hour or so, without internet and possibly without a laptop as well, just carrying an `iPad` with us. Luckily, the `iPad` now has the awesome `Swift Playgrounds` app.

Let's start with a new playground that runs a basic compute kernel. Since the current version of the `Swift Playgrounds` app does not yet let us edit the `Auxiliary Source Files`, where all our `Swift` and `Metal` files usually reside, we will have to write our code in the main playground page, but it is not that complicated. All we have to do is modify our `MetalView` initializer and let it take in an extra argument -- our shader/kernel code. Then we start building our code by adding more lines to this long string.

Let's start with a light blue sky background color:

{% highlight swift %}let shader =
&quot;#include &lt;metal_stdlib&gt;\n&quot; +
&quot;using namespace metal;&quot; +
&quot;kernel void k(texture2d&lt;float,access::write&gt; o[[texture(0)]],&quot; +
&quot;              uint2 gid[[thread_position_in_grid]]) {&quot; +
&quot;   float3 color = float3(0.5, 0.8, 1.0);&quot; +
&quot;   o.write(float4(color, 1.0), gid);&quot; +
&quot;}&quot;
{% endhighlight %}

If you run the playground now, the output image should look like this:

![alt text](https://github.com/MetalKit/images/raw/master/chapter18_1.png &quot;1&quot;)

Next, let's draw a gradient. we divide the current pixel coordinates to the screen dimensions and we get __UV__ -- a pair of floats between __(0-1)__. we then multiply the fixed color with __Y__ -- the vertical component of `UV` which gives us the gradient:

{% highlight swift %}&quot;   int width = o.get_width();&quot; +
&quot;   int height = o.get_height();&quot; +
&quot;   float2 uv = float2(gid) / float2(width, height);&quot; +
&quot;   color *= uv.y;&quot; +
{% endhighlight %}

The output image should look like this:

![alt text](https://github.com/MetalKit/images/raw/master/chapter18_2.png &quot;2&quot;)

Let's work on a nicer background next. A smooth gradient would look like a great sunset. We can use __mix__ to blend colors. We tell the function to blend vertically, and take the complement of __Y__ to switch the colors:

{% highlight swift %}&quot;   float3 color = mix(float3(1.0, 0.6, 0.1), float3(0.5, 0.8, 1.0), sqrt(1 - uv.y));&quot; +
{% endhighlight %}

The output image should look like this:

![alt text](https://github.com/MetalKit/images/raw/master/chapter18_3.png &quot;3&quot;)

From here we could go to drawing a black hole. We would achieve that by using a distance function (__length__) to draw black in the middle of the screen __(0.5, 0.5)__ and add more and more background color outside of it, until we reach the maximum value in the screen corners. Replace the last line with:

{% highlight swift %}&quot;   float2 q = uv - float2(0.5);&quot; +
&quot;   color *= length(q);&quot; +
{% endhighlight %}

The output image should look like this:

![alt text](https://github.com/MetalKit/images/raw/master/chapter18_4.png.png &quot;4&quot;)

Next we use __smootstep__ to draw a round shape that is black inside, blue outside and a blended color between __r__ and __(r + 0.01)__. Replace the last line with:

{% highlight swift %}&quot;   float r = 0.2;&quot; +
&quot;   color *= smoothstep(r, r + 0.01, length(q));&quot; +
{% endhighlight %}

The output image should look like this:

![alt text](https://github.com/MetalKit/images/raw/master/chapter18_5.png &quot;5&quot;)

If we're not satisfied with a circular perimeter, we could make it _bumpy_ by using math functions such as __cos__ and __atan2__. We generate here __9__ spikes (__frequency__) with a spike length (__amplitude__) of __0.1__:

{% highlight swift %}&quot;   float r = 0.2 + 0.1 * cos(atan2(q.x, q.y) * 9.0);&quot; +
{% endhighlight %}

The output image should look like this:

![alt text](https://github.com/MetalKit/images/raw/master/chapter18_6.png &quot;6&quot;)

Adding the __X__ coordinate to the _cosine_ phase introduces a spike _bend-like_ effect:

{% highlight swift %}&quot;   float r = 0.2 + 0.1 * cos(atan2(q.x, q.y) * 9.0 + 20.0 * q.x);&quot; +
{% endhighlight %}

The output image should look like this:

![alt text](https://github.com/MetalKit/images/raw/master/chapter18_7.png &quot;7&quot;)

You can rotate them by adding a small number such as __1.0__ to the _cosine_ value:

{% highlight swift %}&quot;   float r = 0.2 + 0.1 * cos(atan2(q.x, q.y) * 9.0 + 20.0 * q.x + 1.0);&quot; +
{% endhighlight %}

The output image should look like this:

![alt text](https://github.com/MetalKit/images/raw/master/chapter18_8.png &quot;8&quot;)

If you think this is starting to look like a palm tree canopy, I see it too! We can draw its trunk using __abs__ which gives us horizontal/vertical distances instead of _euclidian_ distances (to a given point) like _length_ did, so let's take the __X__ distance and add these lines (we are reusing both __r__ and __color__) after the existing ones: 

{% highlight swift %}&quot;   r = 0.015;&quot; +
&quot;   color *= smoothstep(r, r + 0.002, abs(q.x));&quot; +
{% endhighlight %}

The output image should look like this:

![alt text](https://github.com/MetalKit/images/raw/master/chapter18_9.png &quot;9&quot;)

We can remove the unneeded part of the trunk by using another __smoothstep__ on the __Y__ coordinate:

{% highlight swift %}&quot;   color *= 1.0 - (1.0 - smoothstep(r, r + 0.002, abs(q.x))) * smoothstep(0.0, 0.1, q.y);&quot; +
{% endhighlight %}

The output image should look like this:

![alt text](https://github.com/MetalKit/images/raw/master/chapter18_10.png &quot;10&quot;)

Since both the canopy and trunk are using __q__, modifying its value will move both:

{% highlight swift %}&quot;   float2 q = uv - float2(0.67, 0.29);&quot; +
{% endhighlight %}

The output image should look like this:

![alt text](https://github.com/MetalKit/images/raw/master/chapter18_11.png &quot;11&quot;)

By introducing a __sin__ function we can bend the trunk. A too small _frequency_ does not bend it enough while a too high _frequency_ bends it too much so __2.0__ seems right. An _amplitude_ of __0.25__ also moves the base of the trunk towards the edge of the screen so it looks right (as an aside, changing the sign from __+__ to __--__ will shift the base to the other side):

{% highlight swift %}&quot;   color *= 1.0 - (1.0 - smoothstep(r, r + 0.002, abs(q.x - 0.25 * sin(2.0 * q.y)))) * smoothstep(0.0, 0.1, q.y);&quot; +
{% endhighlight %}

The output image should look like this:

![alt text](https://github.com/MetalKit/images/raw/master/chapter18_12.png &quot;12&quot;)

The trunk, however, is too smooth. To add irregularities to its surface we use __cos__ again. A high _frequency_ and low _amplitude_ seem to be what we need to make it look right:

{% highlight swift %}&quot;   r = 0.015 + 0.002 * cos (120.0 * q.y);&quot; +
{% endhighlight %}

The output image should look like this:

![alt text](https://github.com/MetalKit/images/raw/master/chapter18_13.png &quot;13&quot;)

Also, trunks are usually shaping the ground a little around their base, so an __exp__ function is what we need here because it grows slowly in the beginning and then it soars to the skies. We use an _attenuation_ factor of __-50.0__:

{% highlight swift %}&quot;   r = 0.015 + 0.002 * cos (120.0 * q.y) + exp(-50.0 * (1.0 - uv.y));&quot; +
{% endhighlight %}

The output image should look like this:

![alt text](https://github.com/MetalKit/images/raw/master/chapter18_14.png &quot;14&quot;)

We can increase the presence of the second color by using __sqrt__ which gives us bigger numbers (when used on sub-unitary numbers) to work with. The sunset is about to end soon:

{% highlight swift %}&quot;   float3 color = mix(float3(1.0, 0.6, 0.1), float3(0.5, 0.8, 1.0), sqrt(1 - uv.y));&quot; +
{% endhighlight %}

The final image on the iPad should look like this:

![alt text](https://github.com/MetalKit/images/raw/master/chapter18_15.png &quot;15&quot;)

In conclusion, we saw how to use __sqrt__ to shape transitions, then __cos__ to create variations of ups and downs in shapes, then __exp__ that allows us to create curves, then __smoothstep__ for thresholding, then __abs__ for symmetry and __mix__ for blending. Still commuting? Why don't you take a look at how this nice clover was created:

![alt text](https://github.com/MetalKit/images/raw/master/chapter18_16.png &quot;16&quot;)

I want to say thanks to [Inigo Quilez](https://twitter.com/iquilezles) again, for keep inspiring me to write more and more about drawing with math. All the math in this tutorial belongs to him. The [source code](https://github.com/MetalKit/metal) is posted on `Github` as usual.

Until next time!</content><summary type="html">Yes, as the title suggests, we’re going to have another of those posts with math (and fun) in it. I was thinking the other day, what can we do while commuting for an hour or so, without internet and possibly without a laptop as well, just carrying an iPad with us. Luckily, the iPad now has the awesome Swift Playgrounds app.

Let’s start with a new playground that runs a basic compute kernel. Since the current version of the Swift Playgrounds app does not yet let us edit the Auxiliary Source Files, where all our Swift and Metal files usually reside, we will have to write our code in the main playground page, but it is not that complicated. All we have to do is modify our MetalView initializer and let it take in an extra argument – our shader/kernel code. Then we start building our code by adding more lines to this long string.

Let’s start with a light blue sky background color:

let shader =
&quot;#include &amp;lt;metal_stdlib&amp;gt; &quot; +
&quot;using namespace metal;&quot; +
&quot;kernel void k(texture2d&amp;lt;float,access::write&amp;gt; o[[texture(0)]],&quot; +
&quot;              uint2 gid[[thread_position_in_grid]]) {&quot; +
&quot;   float3 color = float3(0.5, 0.8, 1.0);&quot; +
&quot;   o.write(float4(color, 1.0), gid);&quot; +
&quot;}&quot;

If you run the playground now, the output image should look like this:



Next, let’s draw a gradient. we divide the current pixel coordinates to the screen dimensions and we get UV – a pair of floats between (0-1). we then multiply the fixed color with Y – the vertical component of UV which gives us the gradient:

&quot;   int width = o.get_width();&quot; +
&quot;   int height = o.get_height();&quot; +
&quot;   float2 uv = float2(gid) / float2(width, height);&quot; +
&quot;   color *= uv.y;&quot; +

The output image should look like this:



Let’s work on a nicer background next. A smooth gradient would look like a great sunset. We can use mix to blend colors. We tell the function to blend vertically, and take the complement of Y to switch the colors:

&quot;   float3 color = mix(float3(1.0, 0.6, 0.1), float3(0.5, 0.8, 1.0), sqrt(1 - uv.y));&quot; +

The output image should look like this:



From here we could go to drawing a black hole. We would achieve that by using a distance function (length) to draw black in the middle of the screen (0.5, 0.5) and add more and more background color outside of it, until we reach the maximum value in the screen corners. Replace the last line with:

&quot;   float2 q = uv - float2(0.5);&quot; +
&quot;   color *= length(q);&quot; +

The output image should look like this:



Next we use smootstep to draw a round shape that is black inside, blue outside and a blended color between r and (r + 0.01). Replace the last line with:

&quot;   float r = 0.2;&quot; +
&quot;   color *= smoothstep(r, r + 0.01, length(q));&quot; +

The output image should look like this:



If we’re not satisfied with a circular perimeter, we could make it bumpy by using math functions such as cos and atan2. We generate here 9 spikes (frequency) with a spike length (amplitude) of 0.1:

&quot;   float r = 0.2 + 0.1 * cos(atan2(q.x, q.y) * 9.0);&quot; +

The output image should look like this:



Adding the X coordinate to the cosine phase introduces a spike bend-like effect:

&quot;   float r = 0.2 + 0.1 * cos(atan2(q.x, q.y) * 9.0 + 20.0 * q.x);&quot; +

The output image should look like this:



You can rotate them by adding a small number such as 1.0 to the cosine value:

&quot;   float r = 0.2 + 0.1 * cos(atan2(q.x, q.y) * 9.0 + 20.0 * q.x + 1.0);&quot; +

The output image should look like this:



If you think this is starting to look like a palm tree canopy, I see it too! We can draw its trunk using abs which gives us horizontal/vertical distances instead of euclidian distances (to a given point) like length did, so let’s take the X distance and add these lines (we are reusing both r and color) after the existing ones:

&quot;   r = 0.015;&quot; +
&quot;   color *= smoothstep(r, r + 0.002, abs(q.x));&quot; +

The output image should look like this:



We can remove the unneeded part of the trunk by using another smoothstep on the Y coordinate:

&quot;   color *= 1.0 - (1.0 - smoothstep(r, r + 0.002, abs(q.x))) * smoothstep(0.0, 0.1, q.y);&quot; +

The output image should look like this:



Since both the canopy and trunk are using q, modifying its value will move both:

&quot;   float2 q = uv - float2(0.67, 0.29);&quot; +

The output image should look like this:



By introducing a sin function we can bend the trunk. A too small frequency does not bend it enough while a too high frequency bends it too much so 2.0 seems right. An amplitude of 0.25 also moves the base of the trunk towards the edge of the screen so it looks right (as an aside, changing the sign from + to – will shift the base to the other side):

&quot;   color *= 1.0 - (1.0 - smoothstep(r, r + 0.002, abs(q.x - 0.25 * sin(2.0 * q.y)))) * smoothstep(0.0, 0.1, q.y);&quot; +

The output image should look like this:



The trunk, however, is too smooth. To add irregularities to its surface we use cos again. A high frequency and low amplitude seem to be what we need to make it look right:

&quot;   r = 0.015 + 0.002 * cos (120.0 * q.y);&quot; +

The output image should look like this:



Also, trunks are usually shaping the ground a little around their base, so an exp function is what we need here because it grows slowly in the beginning and then it soars to the skies. We use an attenuation factor of -50.0:

&quot;   r = 0.015 + 0.002 * cos (120.0 * q.y) + exp(-50.0 * (1.0 - uv.y));&quot; +

The output image should look like this:



We can increase the presence of the second color by using sqrt which gives us bigger numbers (when used on sub-unitary numbers) to work with. The sunset is about to end soon:

&quot;   float3 color = mix(float3(1.0, 0.6, 0.1), float3(0.5, 0.8, 1.0), sqrt(1 - uv.y));&quot; +

The final image on the iPad should look like this:



In conclusion, we saw how to use sqrt to shape transitions, then cos to create variations of ups and downs in shapes, then exp that allows us to create curves, then smoothstep for thresholding, then abs for symmetry and mix for blending. Still commuting? Why don’t you take a look at how this nice clover was created:



I want to say thanks to Inigo Quilez again, for keep inspiring me to write more and more about drawing with math. All the math in this tutorial belongs to him. The source code is posted on Github as usual.

Until next time!</summary></entry><entry><title type="html">Using MetalKit part 17</title><link href="http://localhost:4000/2016/09/24/using-metalkit-part-17.html" rel="alternate" type="text/html" title="Using MetalKit part 17" /><published>2016-09-24T00:00:00+03:00</published><updated>2016-09-24T00:00:00+03:00</updated><id>http://localhost:4000/2016/09/24/using-metalkit-part-17</id><content type="html" xml:base="http://localhost:4000/2016/09/24/using-metalkit-part-17.html">I am writing this article for three reasons: first, to tell you that I am working on updating all the `Metal` code to `Swift 3` and then moving the tutorials to a new home with a nicer design and a proper domain name; second, I wanted to show you a different way to work with `MetalKit` other than subclassing `MTKView`, that is, using the `MTKViewDelegate`; and third, I wanted to answer one of our readers' question about how to draw wireframes.

Let's start by using the code from `Part 4` which is an `Xcode` project but we will turn it into a playground this time. This is going to be a shockingly short tutorial but all you have to do is add the following line right before encoding the draw command:

{% highlight swift %}renderEncoder.setTriangleFillMode(.lines)
{% endhighlight %}

That's it! Run the playground and enjoy the wireframed triangle. If you don't want it to have an interpolated color, in the fragment shader also replace the return line with a constant color like green, for example:

{% highlight swift %}return half4(0.0, 1.0, 0.0, 1.0);
{% endhighlight %}

The output image should look like this:

![alt text](https://github.com/MetalKit/images/raw/master/chapter17.png &quot;2D&quot;)

For a `3D` rendering there is one more thing we need to do, disable the backface culling. If you're working in the playground from `Part 9` just comment out this line:

{% highlight swift %}commandEncoder.setCullMode(.back)
{% endhighlight %}

The output image should look like this:

![alt text](https://github.com/MetalKit/images/raw/master/chapter17_2.png &quot;3D&quot;)

The [source code](https://github.com/MetalKit/metal) is posted on `Github` as usual.

Until next time!</content><summary type="html">I am writing this article for three reasons: first, to tell you that I am working on updating all the Metal code to Swift 3 and then moving the tutorials to a new home with a nicer design and a proper domain name; second, I wanted to show you a different way to work with MetalKit other than subclassing MTKView, that is, using the MTKViewDelegate; and third, I wanted to answer one of our readers’ question about how to draw wireframes.

Let’s start by using the code from Part 4 which is an Xcode project but we will turn it into a playground this time. This is going to be a shockingly short tutorial but all you have to do is add the following line right before encoding the draw command:

renderEncoder.setTriangleFillMode(.lines)

That’s it! Run the playground and enjoy the wireframed triangle. If you don’t want it to have an interpolated color, in the fragment shader also replace the return line with a constant color like green, for example:

return half4(0.0, 1.0, 0.0, 1.0);

The output image should look like this:



For a 3D rendering there is one more thing we need to do, disable the backface culling. If you’re working in the playground from Part 9 just comment out this line:

commandEncoder.setCullMode(.back)

The output image should look like this:



The source code is posted on Github as usual.

Until next time!</summary></entry><entry><title type="html">The Model I/O framework</title><link href="http://localhost:4000/2016/08/30/the-model-i-o-framework.html" rel="alternate" type="text/html" title="The Model I/O framework" /><published>2016-08-30T00:00:00+03:00</published><updated>2016-08-30T00:00:00+03:00</updated><id>http://localhost:4000/2016/08/30/the-model-i-o-framework</id><content type="html" xml:base="http://localhost:4000/2016/08/30/the-model-i-o-framework.html">__Model I/O__ was introduced in `2015` for `iOS 9` and `OS X 10.11` and it is a framework that helps us create more realistic and interactive graphics. We can use it to import/export `3D` assets, to describe lighting, materials and environments, to bake lights, to subdivide and voxelize meshes, and for physical based rendering. Model I/O easily integrates our assets with our code in various `3D APIs`:

![alt text](https://github.com/MetalKit/images/raw/master/modelio_1.png &quot;1&quot;)

In order to import an asset we simply do:

{% highlight swift %}var url = URL(string: &quot;/Users/YourUsername/Desktop/imported.obj&quot;)
let asset = MDLAsset(url: url!)
{% endhighlight %}

To export an asset we do:

{% highlight swift %}url = URL(string: &quot;/Users/YourUsername/Desktop/exported.obj&quot;)
try! asset.export(to: url!)
{% endhighlight %}

Model I/O will save both the __.obj__ file and an additional __.mtl__ file that contains information about the object materials, such as in this example:

{% highlight text %}# Apple ModelI/O MTL File: exported.mtl

newmtl material_1
	Kd 0.8 0.8 0.8
	Ka 0 0 0
	Ks 0 0 0
	ao 0 0 0
	subsurface 0 0 0
	metallic 0 0 0
	specularTint 0 0 0
	roughness 0.9 0 0
	anisotropicRotation 0 0 0
	sheen 0.05 0 0
	sheenTint 0 0 0
	clearCoat 0 0 0
	clearCoatGloss 0 0 0
{% endhighlight %}

Integrating `Model I/O` with `Metal` takes four steps:

![alt text](https://github.com/MetalKit/images/raw/master/modelio_2.png &quot;2&quot;)

### Step 1: set up the render pipeline state

First we create a vertex descriptor so we can pass input to the vertex function. The vertex descriptor is needed to describe the vertex attribute inputs to a render state pipeline. We need `3 x 4` bytes for the vertex position, `4 x 1` byte for color, `2 x 2` bytes for the texture coordinates and `4 x 1` byte for ambient occlusion. At the end we tell the descriptor how large (__24__) our `stride` is in total:

{% highlight swift %}let vertexDescriptor = MTLVertexDescriptor()
vertexDescriptor.attributes[0].offset = 0
vertexDescriptor.attributes[0].format = MTLVertexFormat.float3 // position
vertexDescriptor.attributes[1].offset = 12
vertexDescriptor.attributes[1].format = MTLVertexFormat.uChar4 // color
vertexDescriptor.attributes[2].offset = 16
vertexDescriptor.attributes[2].format = MTLVertexFormat.half2 // texture
vertexDescriptor.attributes[3].offset = 20
vertexDescriptor.attributes[3].format = MTLVertexFormat.float // occlusion
vertexDescriptor.layouts[0].stride = 24
let renderPipelineDescriptor = MTLRenderPipelineDescriptor()
renderPipelineDescriptor.vertexDescriptor = vertexDescriptor
let rps = device.newRenderPipelineStateWithDescriptor(renderPipelineDescriptor)
{% endhighlight %}

### Step 2: set up the asset initialization

We need to also create a `Model I/O` vertex descriptor to describe the layout of the vertex attributes in a mesh. We use a model named __Farmhouse.obj__ that also has a texture __Farmhouse.png__ (both already added to the sample project for you):

{% highlight swift %}let desc = MTKModelIOVertexDescriptorFromMetal(vertexDescriptor)
var attribute = desc.attributes[0] as! MDLVertexAttribute
attribute.name = MDLVertexAttributePosition
attribute = desc.attributes[1] as! MDLVertexAttribute
attribute.name = MDLVertexAttributeColor
attribute = desc.attributes[2] as! MDLVertexAttribute
attribute.name = MDLVertexAttributeTextureCoordinate
attribute = desc.attributes[3] as! MDLVertexAttribute
attribute.name = MDLVertexAttributeOcclusionValue
let mtkBufferAllocator = MTKMeshBufferAllocator(device: device!)
let url = Bundle.main.url(forResource: &quot;Farmhouse&quot;, withExtension: &quot;obj&quot;)
let asset = MDLAsset(url: url!, vertexDescriptor: desc, bufferAllocator: mtkBufferAllocator)
{% endhighlight %}

Next, we load the texture for our asset:

{% highlight swift %}let loader = MTKTextureLoader(device: device)
let file = Bundle.main.path(forResource: &quot;Farmhouse&quot;, ofType: &quot;png&quot;)
let data = try Data(contentsOf: URL(fileURLWithPath: file))
let texture = try loader.newTexture(with: data, options: nil)
{% endhighlight %}

### Step 3: set up `MetalKit` mesh and submesh objects

We are now creating the meshes and submeshes needed in the final, fourth step. We also compute the __Ambient Occlusion__, which is a measure of geometry obstruction, and it tells us how much of the ambient light actually reaches any given pixel or point of our object, and and how much of this light is blocked by surrounding meshes. `Model I/O` provides a `UV` mapper that creates a `2D` texture and wraps it around the object's `3D` mesh. For every pixel in the texture we can compute the ambient occlusion value, which is just one extra float added for each vertex:

{% highlight swift %}let mesh = asset.object(at: 0) as? MDLMesh
mesh.generateAmbientOcclusionVertexColors(withQuality: 1, attenuationFactor: 0.98, objectsToConsider: [mesh], vertexAttributeNamed: MDLVertexAttributeOcclusionValue)
let meshes = try MTKMesh.newMeshes(from: asset, device: device!, sourceMeshes: nil)
{% endhighlight %}

### Step 4: set up `Metal` rendering and drawing of meshes

Finally, we configure the command encoder with the mesh data that it needs to draw:

{% highlight swift %}let mesh = (meshes?.first)!
let vertexBuffer = mesh.vertexBuffers[0]
commandEncoder.setVertexBuffer(vertexBuffer.buffer, offset: vertexBuffer.offset, at: 0)
let submesh = mesh.submeshes.first!
commandEncoder.drawIndexedPrimitives(submesh.primitiveType, indexCount: submesh.indexCount, indexType: submesh.indexType, indexBuffer: submesh.indexBuffer.buffer, indexBufferOffset: submesh.indexBuffer.offset)
{% endhighlight %}

Next, we will work on our shader functions. First we set up our structs for the vertices and uniforms:

{% highlight swift %}struct VertexIn {
    float4 position [[attribute(0)]];
    float4 color [[attribute(1)]];
    float2 texCoords [[attribute(2)]];
    float occlusion [[attribute(3)]];
};

struct VertexOut {
    float4 position [[position]];
    float4 color;
    float2 texCoords;
    float occlusion;
};

struct Uniforms {
    float4x4 modelViewProjectionMatrix;
};
{% endhighlight %}

Notice that we are matching the information we set up in the vertex descriptor, with the `VertexIn` struct.
For the vertex function, we use a __[[stage_in]]__ attribute because we are passing per-vertex inputs as an argument to this function:

{% highlight swift %}vertex VertexOut vertex_func(const VertexIn vertices [[stage_in]],
                             constant Uniforms &amp;uniforms [[buffer(1)]],
                             uint vertexId [[vertex_id]])
{
    float4x4 mvpMatrix = uniforms.modelViewProjectionMatrix;
    float4 position = vertices.position;
    VertexOut out;
    out.position = mvpMatrix * position;
    out.color = float4(1);
    out.texCoords = vertices.texCoords;
    out.occlusion = vertices.occlusion;
    return out;
}
{% endhighlight %}

The fragment function reads the per-fragment inputs passed from the vertex function and also processes the texture we passed via the command encoder: 

{% highlight swift %}fragment half4 fragment_func(VertexOut fragments [[stage_in]],
                             texture2d&lt;float&gt; textures [[texture(0)]])
{
    float4 baseColor = fragments.color;
    return half4(baseColor);
}
{% endhighlight %}

If you run the playground, you will see this output image:

![alt text](https://github.com/MetalKit/images/raw/master/modelio_3.png &quot;3&quot;)

That's a pretty dull white model. Let's apply the ambient occlusion to it by replacing the last line in the fragment function with these lines:

{% highlight swift %}float4 occlusion = fragments.occlusion;
return half4(baseColor * occlusion);
{% endhighlight %}

If you run the playground again, you will see this output image:

![alt text](https://github.com/MetalKit/images/raw/master/modelio_4.png &quot;4&quot;)

The ambient occlusion also seems a bit raw and that is because our model is quite flat, without any curves or surface irregularities which would give way more credit to the realism that the ambient occlusion brings. Next, let's apply the texture. Replace the last line in the fragment function with these lines: 

{% highlight swift %}constexpr sampler samplers;
float4 texture = textures.sample(samplers, fragments.texCoords);
return half4(baseColor * texture);
{% endhighlight %}    

If you run the playground again, you will see this output image:

![alt text](https://github.com/MetalKit/images/raw/master/modelio_5.png &quot;5&quot;)

The texture looks really great on this model, but it would look even more realistic if we brought the ambient occlusion back. Replace the last line in the fragment function with this line: 

{% highlight swift %}return half4(baseColor * occlusion * texture);
{% endhighlight %}

If you run the playground again, you will see this output image:

![alt text](https://github.com/MetalKit/images/raw/master/modelio_6.png &quot;6&quot;)

Not bad for a few lines of code, right? `Model I/O` is such a great framework for `3D` graphics and game programmers. There are a couple of articles on the web about using `Model I/O` with `SceneKit`, however, I thought using it with `Metal` is even more interesting! The [source code](https://github.com/MetalKit/modelio) is posted on Github as usual.

Until next time!</content><summary type="html">Model I/O was introduced in 2015 for iOS 9 and OS X 10.11 and it is a framework that helps us create more realistic and interactive graphics. We can use it to import/export 3D assets, to describe lighting, materials and environments, to bake lights, to subdivide and voxelize meshes, and for physical based rendering. Model I/O easily integrates our assets with our code in various 3D APIs:



In order to import an asset we simply do:

var url = URL(string: &quot;/Users/YourUsername/Desktop/imported.obj&quot;)
let asset = MDLAsset(url: url!)

To export an asset we do:

url = URL(string: &quot;/Users/YourUsername/Desktop/exported.obj&quot;)
try! asset.export(to: url!)

Model I/O will save both the .obj file and an additional .mtl file that contains information about the object materials, such as in this example:

# Apple ModelI/O MTL File: exported.mtl

newmtl material_1
	Kd 0.8 0.8 0.8
	Ka 0 0 0
	Ks 0 0 0
	ao 0 0 0
	subsurface 0 0 0
	metallic 0 0 0
	specularTint 0 0 0
	roughness 0.9 0 0
	anisotropicRotation 0 0 0
	sheen 0.05 0 0
	sheenTint 0 0 0
	clearCoat 0 0 0
	clearCoatGloss 0 0 0

Integrating Model I/O with Metal takes four steps:



Step 1: set up the render pipeline state

First we create a vertex descriptor so we can pass input to the vertex function. The vertex descriptor is needed to describe the vertex attribute inputs to a render state pipeline. We need 3 x 4 bytes for the vertex position, 4 x 1 byte for color, 2 x 2 bytes for the texture coordinates and 4 x 1 byte for ambient occlusion. At the end we tell the descriptor how large (24) our stride is in total:

let vertexDescriptor = MTLVertexDescriptor()
vertexDescriptor.attributes[0].offset = 0
vertexDescriptor.attributes[0].format = MTLVertexFormat.float3 // position

vertexDescriptor.attributes[1].offset = 12
vertexDescriptor.attributes[1].format = MTLVertexFormat.uChar4 // color

vertexDescriptor.attributes[2].offset = 16
vertexDescriptor.attributes[2].format = MTLVertexFormat.half2 // texture

vertexDescriptor.attributes[3].offset = 20
vertexDescriptor.attributes[3].format = MTLVertexFormat.float // occlusion

vertexDescriptor.layouts[0].stride = 24
let renderPipelineDescriptor = MTLRenderPipelineDescriptor()
renderPipelineDescriptor.vertexDescriptor = vertexDescriptor
let rps = device.newRenderPipelineStateWithDescriptor(renderPipelineDescriptor)

Step 2: set up the asset initialization

We need to also create a Model I/O vertex descriptor to describe the layout of the vertex attributes in a mesh. We use a model named Farmhouse.obj that also has a texture Farmhouse.png (both already added to the sample project for you):

let desc = MTKModelIOVertexDescriptorFromMetal(vertexDescriptor)
var attribute = desc.attributes[0] as! MDLVertexAttribute
attribute.name = MDLVertexAttributePosition
attribute = desc.attributes[1] as! MDLVertexAttribute
attribute.name = MDLVertexAttributeColor
attribute = desc.attributes[2] as! MDLVertexAttribute
attribute.name = MDLVertexAttributeTextureCoordinate
attribute = desc.attributes[3] as! MDLVertexAttribute
attribute.name = MDLVertexAttributeOcclusionValue
let mtkBufferAllocator = MTKMeshBufferAllocator(device: device!)
let url = Bundle.main.url(forResource: &quot;Farmhouse&quot;, withExtension: &quot;obj&quot;)
let asset = MDLAsset(url: url!, vertexDescriptor: desc, bufferAllocator: mtkBufferAllocator)

Next, we load the texture for our asset:

let loader = MTKTextureLoader(device: device)
let file = Bundle.main.path(forResource: &quot;Farmhouse&quot;, ofType: &quot;png&quot;)
let data = try Data(contentsOf: URL(fileURLWithPath: file))
let texture = try loader.newTexture(with: data, options: nil)

Step 3: set up MetalKit mesh and submesh objects

We are now creating the meshes and submeshes needed in the final, fourth step. We also compute the Ambient Occlusion, which is a measure of geometry obstruction, and it tells us how much of the ambient light actually reaches any given pixel or point of our object, and and how much of this light is blocked by surrounding meshes. Model I/O provides a UV mapper that creates a 2D texture and wraps it around the object’s 3D mesh. For every pixel in the texture we can compute the ambient occlusion value, which is just one extra float added for each vertex:

let mesh = asset.object(at: 0) as? MDLMesh
mesh.generateAmbientOcclusionVertexColors(withQuality: 1, attenuationFactor: 0.98, objectsToConsider: [mesh], vertexAttributeNamed: MDLVertexAttributeOcclusionValue)
let meshes = try MTKMesh.newMeshes(from: asset, device: device!, sourceMeshes: nil)

Step 4: set up Metal rendering and drawing of meshes

Finally, we configure the command encoder with the mesh data that it needs to draw:

let mesh = (meshes?.first)!
let vertexBuffer = mesh.vertexBuffers[0]
commandEncoder.setVertexBuffer(vertexBuffer.buffer, offset: vertexBuffer.offset, at: 0)
let submesh = mesh.submeshes.first!
commandEncoder.drawIndexedPrimitives(submesh.primitiveType, indexCount: submesh.indexCount, indexType: submesh.indexType, indexBuffer: submesh.indexBuffer.buffer, indexBufferOffset: submesh.indexBuffer.offset)

Next, we will work on our shader functions. First we set up our structs for the vertices and uniforms:

struct VertexIn {
    float4 position [[attribute(0)]];
    float4 color [[attribute(1)]];
    float2 texCoords [[attribute(2)]];
    float occlusion [[attribute(3)]];
};

struct VertexOut {
    float4 position [[position]];
    float4 color;
    float2 texCoords;
    float occlusion;
};

struct Uniforms {
    float4x4 modelViewProjectionMatrix;
};

Notice that we are matching the information we set up in the vertex descriptor, with the VertexIn struct.
For the vertex function, we use a [[stage_in]] attribute because we are passing per-vertex inputs as an argument to this function:

vertex VertexOut vertex_func(const VertexIn vertices [[stage_in]],
                             constant Uniforms &amp;amp;uniforms [[buffer(1)]],
                             uint vertexId [[vertex_id]])
{
    float4x4 mvpMatrix = uniforms.modelViewProjectionMatrix;
    float4 position = vertices.position;
    VertexOut out;
    out.position = mvpMatrix * position;
    out.color = float4(1);
    out.texCoords = vertices.texCoords;
    out.occlusion = vertices.occlusion;
    return out;
}

The fragment function reads the per-fragment inputs passed from the vertex function and also processes the texture we passed via the command encoder:

fragment half4 fragment_func(VertexOut fragments [[stage_in]],
                             texture2d&amp;lt;float&amp;gt; textures [[texture(0)]])
{
    float4 baseColor = fragments.color;
    return half4(baseColor);
}

If you run the playground, you will see this output image:



That’s a pretty dull white model. Let’s apply the ambient occlusion to it by replacing the last line in the fragment function with these lines:

float4 occlusion = fragments.occlusion;
return half4(baseColor * occlusion);

If you run the playground again, you will see this output image:



The ambient occlusion also seems a bit raw and that is because our model is quite flat, without any curves or surface irregularities which would give way more credit to the realism that the ambient occlusion brings. Next, let’s apply the texture. Replace the last line in the fragment function with these lines:

constexpr sampler samplers;
float4 texture = textures.sample(samplers, fragments.texCoords);
return half4(baseColor * texture);

If you run the playground again, you will see this output image:



The texture looks really great on this model, but it would look even more realistic if we brought the ambient occlusion back. Replace the last line in the fragment function with this line:

return half4(baseColor * occlusion * texture);

If you run the playground again, you will see this output image:



Not bad for a few lines of code, right? Model I/O is such a great framework for 3D graphics and game programmers. There are a couple of articles on the web about using Model I/O with SceneKit, however, I thought using it with Metal is even more interesting! The source code is posted on Github as usual.

Until next time!</summary></entry><entry><title type="html">Ray tracing in a Swift playground part 6</title><link href="http://localhost:4000/2016/08/07/ray-tracing-in-a-swift-playground-part-6.html" rel="alternate" type="text/html" title="Ray tracing in a Swift playground part 6" /><published>2016-08-07T00:00:00+03:00</published><updated>2016-08-07T00:00:00+03:00</updated><id>http://localhost:4000/2016/08/07/ray-tracing-in-a-swift-playground-part-6</id><content type="html" xml:base="http://localhost:4000/2016/08/07/ray-tracing-in-a-swift-playground-part-6.html">Today, I am looking again at the `Ray Tracing` project because I wanted to see how it runs in an `iPad Playground`. There aren't any changes in the core code for now, except I have updated it to run on iOS 10, Xcode 8, Swift 3 and the new iPad Playground app. 

If you run the playground, now you have the option to play with the __number of samples (ns)__ right on the main page of the playground. Fair warning, the higher you set that number, the longer it will take your playground to finish running but the higher the quality of the output image will be. The runtime will also increase if you make the `width` or `height` bigger. For `400 x 200` and `ns = 10`, you will get an image like this:

![alt text](https://github.com/MetalKit/images/raw/master/raytracing_01.png &quot;1&quot;)

In order to get the image to show, you need to tap on the little icon that looks like an image, at the end of the line, and choose `Add viewer`. You could amp up the resolution to say, `800 x 400`, but this will also increase the running time of your playground. However, the output image is worth the waiting!

![alt text](https://github.com/MetalKit/images/raw/master/raytracing_02.png &quot;2&quot;)

We will soon look at ways to make the playground run faster and generate higher quality output images. My good friend and scientific programming guru, [Jeff](https://twitter.com/hyperjeff/), is working on a `Metal`-based version of this raytracer. We will talk about that soon, too. The [source code](https://github.com/MetalKit/raytracing) for the playground in this article is posted on Github as usual.

Until next time!</content><summary type="html">Today, I am looking again at the Ray Tracing project because I wanted to see how it runs in an iPad Playground. There aren’t any changes in the core code for now, except I have updated it to run on iOS 10, Xcode 8, Swift 3 and the new iPad Playground app.

If you run the playground, now you have the option to play with the number of samples (ns) right on the main page of the playground. Fair warning, the higher you set that number, the longer it will take your playground to finish running but the higher the quality of the output image will be. The runtime will also increase if you make the width or height bigger. For 400 x 200 and ns = 10, you will get an image like this:



In order to get the image to show, you need to tap on the little icon that looks like an image, at the end of the line, and choose Add viewer. You could amp up the resolution to say, 800 x 400, but this will also increase the running time of your playground. However, the output image is worth the waiting!



We will soon look at ways to make the playground run faster and generate higher quality output images. My good friend and scientific programming guru, Jeff, is working on a Metal-based version of this raytracer. We will talk about that soon, too. The source code for the playground in this article is posted on Github as usual.

Until next time!</summary></entry><entry><title type="html">Metal Performance Shaders for the iPad playground</title><link href="http://localhost:4000/2016/07/31/metal-performance-shaders-for-the-ipad-playground.html" rel="alternate" type="text/html" title="Metal Performance Shaders for the iPad playground" /><published>2016-07-31T00:00:00+03:00</published><updated>2016-07-31T00:00:00+03:00</updated><id>http://localhost:4000/2016/07/31/metal-performance-shaders-for-the-ipad-playground</id><content type="html" xml:base="http://localhost:4000/2016/07/31/metal-performance-shaders-for-the-ipad-playground.html">As many of you might have seen at `WWDC 2016`, the new [Playground app for the iPad](https://developer.apple.com/videos/play/wwdc2016/408/) was a really big hit! As an already playgrounds lover, for me it was even more than that. Now we are able to easily write `Swift` code on the iPad and run it with just a button tap. Today we are going to have fun with __Metal Performance Shaders (MPS)__ since I have not discussed about them before, and also because it is so easy to use them on mobile devices. This reminds me to warn you, the `MPS` framework only works on `iOS` and `tvOS` devices. We will look into a handy way of writing the playground on a `macOS` device and then sharing it with your mobile device through the `iCloud Drive`. Also, the `iPad` playgrounds are working only on `iOS 10` or newer.

To start, let's create a new `iOS` playground in `Xcode`. You can add any image you like under the `Resources` folder. I already added one named __nature.jpg__. Next, write a few lines of code in the main playground page, like in this screenshot (the code is available on Github):

![alt text](https://github.com/MetalKit/images/raw/master/mps_1.png &quot;1&quot;)

Let's go over the code. We have been writing most of this code over and over in the previous blog posts. The first new addition you will notice immediately is also generating an error, and that is because `macOS` does not &quot;know&quot; about the `MPS` framework. Once we load this playground on an iPad, the error will go away:

{% highlight swift %}import MetalPerformanceShaders
{% endhighlight %}

Next, we use `MTKTextureLoader` to create a new texture from the image we added above. Now comes the really interesting part! Once we created our `MTLCommandBuffer` object, we are not going to create a `MTLCommandEncoder` object too from this command buffer as we were used to do. Rather, we create a new `MPSImageGaussianBlur` object as in the code below:

{% highlight swift %}let shader = MPSImageGaussianBlur(device: view.device!, sigma: 5)
shader.encode(commandBuffer: commandBuffer, sourceTexture: texIn, destinationTexture: texOut)
{% endhighlight %}

What's great about the `MPS` objects is that they let you apply a compute shader (kernel function) to an input texture without us even having to configure any states, descriptors, pipelines or even write a kernel function, ever! The `MPS` object takes care of everything for us. Of course, by taking this approach we are somewhat limited to only picking a preset shader and possibly changing a parameter such as __sigma__, for this particular shader.

So far so good! We are now ready to send our playground to the `iPad` through the `iCloud Drive`. Open a `Finder` window, click on `iCloud Drive` and copy your playground into this folder:

![alt text](https://github.com/MetalKit/images/raw/master/mps_8.PNG &quot;2&quot;)

We are finally getting to use the `iPad`! Open the new `Playgrounds` app that you downloaded from the `App Store` and go to `My Playgrounds`. In the top left corner of the screen, tap on the __+__ button. As you can see you can also create a new playground on the `iPad` and then easily export it to your `macOS` devices or share it using various other tools. For now, however, we are going to tap on `iCloud Drive` as seen below:

![alt text](https://github.com/MetalKit/images/raw/master/mps_2.PNG &quot;2&quot;)

When the `iCloud Drive` window pops up, notice that we have access to the `iPad`'s private folder for playgrounds as well, however, we now want to import the one we were working on, __MPS.playground__, so tap on it:

![alt text](https://github.com/MetalKit/images/raw/master/mps_3.PNG &quot;3&quot;)

As soon as the `iPad` loaded the playground, we're in business! You can see the main playground page now on your `iPad`:

![alt text](https://github.com/MetalKit/images/raw/master/mps_4.PNG &quot;4&quot;)

All you have to do now is tap on `Run My Code` and see our image with a nice blur filter applied to it:

![alt text](https://github.com/MetalKit/images/raw/master/mps_5.PNG &quot;5&quot;)

&quot;Ok,&quot; you might say, &quot;but how do we see the entire image?&quot; The answer is in the animated GIF below. It's as easy as long pressing in the middle of the screen until a screen separating line shows. With the finger still on the screen, drag the line to either the left side or the right side, depending on what do you need to see, your code or your output image (you might want to reload the page as the animated GIF may only run once before it stops):

![alt text](https://github.com/MetalKit/images/raw/master/mps_6.gif &quot;6&quot;)

Now you can see that blurred image entirely! And when you're done admiring the image, tap that handy button on the left side (and midway vertically) of the screen to go back to split screen. Before we wrap it up, one more piece of awesomeness. Replace the line below:

{% highlight swift %}let shader = MPSImageGaussianBlur(device: view.device!, sigma: 5)
{% endhighlight %}

with this line:

{% highlight swift %}let shader = MPSImageSobel(device: device)
{% endhighlight %}

Check out the new output image:

![alt text](https://github.com/MetalKit/images/raw/master/mps_7.PNG &quot;7&quot;)

Imagine the possibilities you have here by only changing one line of code! There are a couple dozen different shaders that you can try. Check out the [Metal Performance Shaders API](https://developer.apple.com/reference/metalperformanceshaders#symbols) for more details. If you are interested in image processing, you might want to also check Simon Gladman's [Core Image for Swift](https://itunes.apple.com/us/book/core-image-for-swift/id1073029980?mt=13) book. If you want to learn more about the `Metal` backend of `MPS`, also check out Warren Moore's [Metal by Example](https://gum.co/metalbyexample) book. The [source code](https://github.com/MetalKit/mps) for the playground in this article is posted on Github as usual.

Until next time!</content><summary type="html">As many of you might have seen at WWDC 2016, the new Playground app for the iPad was a really big hit! As an already playgrounds lover, for me it was even more than that. Now we are able to easily write Swift code on the iPad and run it with just a button tap. Today we are going to have fun with Metal Performance Shaders (MPS) since I have not discussed about them before, and also because it is so easy to use them on mobile devices. This reminds me to warn you, the MPS framework only works on iOS and tvOS devices. We will look into a handy way of writing the playground on a macOS device and then sharing it with your mobile device through the iCloud Drive. Also, the iPad playgrounds are working only on iOS 10 or newer.

To start, let’s create a new iOS playground in Xcode. You can add any image you like under the Resources folder. I already added one named nature.jpg. Next, write a few lines of code in the main playground page, like in this screenshot (the code is available on Github):



Let’s go over the code. We have been writing most of this code over and over in the previous blog posts. The first new addition you will notice immediately is also generating an error, and that is because macOS does not “know” about the MPS framework. Once we load this playground on an iPad, the error will go away:

import MetalPerformanceShaders

Next, we use MTKTextureLoader to create a new texture from the image we added above. Now comes the really interesting part! Once we created our MTLCommandBuffer object, we are not going to create a MTLCommandEncoder object too from this command buffer as we were used to do. Rather, we create a new MPSImageGaussianBlur object as in the code below:

let shader = MPSImageGaussianBlur(device: view.device!, sigma: 5)
shader.encode(commandBuffer: commandBuffer, sourceTexture: texIn, destinationTexture: texOut)

What’s great about the MPS objects is that they let you apply a compute shader (kernel function) to an input texture without us even having to configure any states, descriptors, pipelines or even write a kernel function, ever! The MPS object takes care of everything for us. Of course, by taking this approach we are somewhat limited to only picking a preset shader and possibly changing a parameter such as sigma, for this particular shader.

So far so good! We are now ready to send our playground to the iPad through the iCloud Drive. Open a Finder window, click on iCloud Drive and copy your playground into this folder:



We are finally getting to use the iPad! Open the new Playgrounds app that you downloaded from the App Store and go to My Playgrounds. In the top left corner of the screen, tap on the + button. As you can see you can also create a new playground on the iPad and then easily export it to your macOS devices or share it using various other tools. For now, however, we are going to tap on iCloud Drive as seen below:



When the iCloud Drive window pops up, notice that we have access to the iPad’s private folder for playgrounds as well, however, we now want to import the one we were working on, MPS.playground, so tap on it:



As soon as the iPad loaded the playground, we’re in business! You can see the main playground page now on your iPad:



All you have to do now is tap on Run My Code and see our image with a nice blur filter applied to it:



“Ok,” you might say, “but how do we see the entire image?” The answer is in the animated GIF below. It’s as easy as long pressing in the middle of the screen until a screen separating line shows. With the finger still on the screen, drag the line to either the left side or the right side, depending on what do you need to see, your code or your output image (you might want to reload the page as the animated GIF may only run once before it stops):



Now you can see that blurred image entirely! And when you’re done admiring the image, tap that handy button on the left side (and midway vertically) of the screen to go back to split screen. Before we wrap it up, one more piece of awesomeness. Replace the line below:

let shader = MPSImageGaussianBlur(device: view.device!, sigma: 5)

with this line:

let shader = MPSImageSobel(device: device)

Check out the new output image:



Imagine the possibilities you have here by only changing one line of code! There are a couple dozen different shaders that you can try. Check out the Metal Performance Shaders API for more details. If you are interested in image processing, you might want to also check Simon Gladman’s Core Image for Swift book. If you want to learn more about the Metal backend of MPS, also check out Warren Moore’s Metal by Example book. The source code for the playground in this article is posted on Github as usual.

Until next time!</summary></entry><entry><title type="html">Using MetalKit part 16</title><link href="http://localhost:4000/2016/07/06/using-metalkit-part-16.html" rel="alternate" type="text/html" title="Using MetalKit part 16" /><published>2016-07-06T00:00:00+03:00</published><updated>2016-07-06T00:00:00+03:00</updated><id>http://localhost:4000/2016/07/06/using-metalkit-part-16</id><content type="html" xml:base="http://localhost:4000/2016/07/06/using-metalkit-part-16.html">A couple of weeks ago, at the `WWDC 2016`, the `Apple` engineers released a new document, the __Metal Best Practices Guide__ which includes useful information about organizing your code for better performance in your `Metal` apps. Because the documentation is quite extensive, we are just going to outline the main concepts in this article. An efficient `Metal` app requires:

- Low `CPU` overhead.
- Optimal `GPU` performance.
- Continuous processor parallelism. 
- Effective resource management. 


## __1 Resource Management__

### _1.1 Persistent Objects_

__Best Practice__: Create persistent objects early and reuse them often.

The `Metal` framework provides several protocols to manage persistent objects throughout the lifetime of your app. These objects are expensive to create but are usually initialized once and reused often. Do not create these objects at the beginning of every render or compute loop.

- Initialize Your Device and Command Queue First
- Compile Your Functions and Build Your Library at Build Time
- Build Your Pipelines Once and Reuse Them Often
- Allocate Resource Storage Up Front

For more information, consult the [Persistent Objects](https://developer.apple.com/library/prerelease/content/documentation/3DDrawing/Conceptual/MTLBestPracticesGuide/PersistentObjects.html) section of the documentation. 

### _1.2 Resource Options_

__Best Practice__: Set appropriate resource storage modes and texture usage options for your resources.

`Metal` resources must be configured appropriately to take advantage of fast memory access and driver performance optimizations. Resource storage modes allow you to define the storage location and access permissions for your `MTLBuffer` and `MTLTexture` objects. Texture usage options allow you to explicitly declare how you intend to use your `MTLTexture` objects.

- Familiarize Yourself with Device Memory Models
- Choose an Appropriate Resource Storage Mode (`iOS` and `tvOS`)
- Choose an Appropriate Resource Storage Mode (`OS X`)
- Set Appropriate Texture Usage Flags

For more information, consult the [Resource Options](https://developer.apple.com/library/prerelease/content/documentation/3DDrawing/Conceptual/MTLBestPracticesGuide/ResourceOptions.html) section of the documentation. 

### _1.3 Triple Buffering_

__Best Practice__: Implement a triple buffering model to update dynamic buffer data.

Dynamic buffer data refers to frequently-updated data stored in a buffer. To avoid creating new buffers per frame and to minimize processor idle time between frames, implementing a triple buffering model is strongly recommended.

- Prevent Access Conflicts and Reduce Processor Idle Time
- Reduce Memory Overhead and Frame Latency
- Allow Time for Command Buffer Transactions
- Implement a Triple Buffering Model

For more information, consult the [Triple Buffering](https://developer.apple.com/library/prerelease/content/documentation/3DDrawing/Conceptual/MTLBestPracticesGuide/TripleBuffering.html) section of the documentation. 

### _1.4 Buffer Bindings_

__Best Practice__: Use an appropriate method to bind your buffer data to a graphics or compute function.

`Metal` provides several `API` options for binding buffer data to a graphics or compute function. The __setVertexBytes:length:atIndex:__ method is the best option for binding an amount of dynamic buffer data (a transient buffer) that is less than __4 KB__ to a vertex function. If the data size is larger than 4 KB, you should create a __MTLBuffer__ once and update its contents as needed. 

For more information, consult the [Buffer Bindings](https://developer.apple.com/library/prerelease/content/documentation/3DDrawing/Conceptual/MTLBestPracticesGuide/BufferBindings.html) section of the documentation. 


## __2 Display Management__

### _2.1 Drawables_

__Best Practice__: Hold a drawable as briefly as possible.

The command buffer is used to schedule a drawable's presentation with the __presentDrawable:__ method before the command buffer itself is scheduled for execution, however, the drawable itself is actually presented after the command buffer has completed execution.

- Use a MetalKit View to Acquire a Drawable

For more information, consult the [Drawables](https://developer.apple.com/library/prerelease/content/documentation/3DDrawing/Conceptual/MTLBestPracticesGuide/Drawables.html) section of the documentation. 

### _2.2 Native Screen Scale (iOS and tvOS)_

__Best Practice__: Render at the exact pixel size of your target display.

The pixel size of your drawables should always match the exact pixel size of their target display. This is critical to avoid rendering to off-screen pixels or incurring an additional sampling stage.

- Use a MetalKit View to Support Native Screen Scale

For more information, consult the [Native Screen Scale](https://developer.apple.com/library/prerelease/content/documentation/3DDrawing/Conceptual/MTLBestPracticesGuide/NativeScreenScale.html) section of the documentation. 

### _2.3 Frame Rate (iOS and tvOS)_

__Best Practice__: For apps that can't maintain a __60 FPS__ frame rate, present your drawables at a steady frame rate.

The display refresh rate of `iOS` devices is `60 Hz`. Apps that are consistently unable to complete a frame's work within this time should target a lower frame rate to avoid jitter. The display refresh rate of `tvOS` devices is usually, but not always, `60 Hz`.

- Use the Display Link
- Adjust the Frame Interval
- Adjust the Drawable Presentation Time

For more information, consult the [Frame Rate](https://developer.apple.com/library/prerelease/content/documentation/3DDrawing/Conceptual/MTLBestPracticesGuide/FrameRate.html) section of the documentation. 


## __3 Command Generation__

### _3.1 Load and Store Actions_

__Best Practice__: Set appropriate load and store actions for your render targets.

Actions performed on your `Metal` render targets must be configured appropriately to avoid costly and unnecessary rendering work at the start (load action) or end (store action) of a rendering pass.

- Choose an Appropriate Load Action
- Choose an Appropriate Store Action
- Evaluate Actions Between Rendering Passes

For more information, consult the [Load and Store Actions](https://developer.apple.com/library/prerelease/content/documentation/3DDrawing/Conceptual/MTLBestPracticesGuide/LoadandStoreActions.html) section of the documentation. 

### _3.2 Render Command Encoders (iOS and tvOS)_

__Best Practice__: Merge render command encoders when possible.

Eliminating unnecessary render command encoders reduces memory bandwidth and increases performance. 

- Evaluate Rendering Pass Order
- Evaluate Sampling Dependencies
- Evaluate Actions Between Rendering Passes

For more information, consult the [Render Command Encoders](https://developer.apple.com/library/prerelease/content/documentation/3DDrawing/Conceptual/MTLBestPracticesGuide/RenderCommandEncoders.html) section of the documentation. 

### _3.3 Command Buffers_

__Best Practice__: Submit the fewest command buffers per frame without underutilizing the `GPU`.

Command buffers are the unit of work submission in `Metal`; they are created by the `CPU` and executed by the `GPU`. This relationship allows you to balance `CPU` and `GPU` work by adjusting the number of command buffers submitted per frame.

For more information, consult the [Command Buffers](https://developer.apple.com/library/prerelease/content/documentation/3DDrawing/Conceptual/MTLBestPracticesGuide/CommandBuffers.html) section of the documentation. 

### _3.4 Indirect Buffers_

__Best Practice__: Use indirect buffers if your draw or dispatch call arguments are dynamically generated by the `GPU`.

Indirect buffers are `MTLBuffer` objects with a specific data layout representing draw or dispatch call arguments. 

- Eliminate Unnecessary Data Transfers and Reduce Processor Idle Time

For more information, consult the [Indirect Buffers](https://developer.apple.com/library/prerelease/content/documentation/3DDrawing/Conceptual/MTLBestPracticesGuide/IndirectBuffers.html) section of the documentation. 


## __4 Compilation__

### _4.1 Functions and Libraries_

__Best Practice__: Compile your functions and build your library at build time.

Compiling `Metal Shading Language` source code is one of the most expensive stages in a `Metal` app. `Metal` is designed to minimize this cost by allowing you to compile graphics and compute functions at build time, then load them at runtime as a library.

- Build Your Library at Build Time
- Group Your Functions into a Single Library

For more information, consult the [Functions and Libraries](https://developer.apple.com/library/prerelease/content/documentation/3DDrawing/Conceptual/MTLBestPracticesGuide/FunctionsandLibraries.html) section of the documentation. 

### _4.2 Pipelines_

__Best Practice__: Build your render and compute pipelines asynchronously.

Having multiple render or compute pipelines allows your app to use different state configurations for specific tasks. Building these pipelines asynchronously maximizes performance and parallelism. It is recommended that you build all known pipelines up front and avoid lazy loading. 

For more information, consult the [Pipelines](https://developer.apple.com/library/prerelease/content/documentation/3DDrawing/Conceptual/MTLBestPracticesGuide/Pipelines.html) section of the documentation. 


This guide, along with the [__Metal Programming Guide__](https://developer.apple.com/library/prerelease/content/documentation/Miscellaneous/Conceptual/MetalProgrammingGuide/Introduction/Introduction.html) and the [__Metal Shading Language Guide__](https://developer.apple.com/library/prerelease/content/documentation/Metal/Reference/MetalShadingLanguageGuide/Introduction/Introduction.html) both already updated for `iOS 10`, `tvOS 10` and `OS X 10.12`, give you the trilogy of documents containing everything you need to start creating performant `Metal` apps. 

Until next time!</content><summary type="html">A couple of weeks ago, at the WWDC 2016, the Apple engineers released a new document, the Metal Best Practices Guide which includes useful information about organizing your code for better performance in your Metal apps. Because the documentation is quite extensive, we are just going to outline the main concepts in this article. An efficient Metal app requires:


  Low CPU overhead.
  Optimal GPU performance.
  Continuous processor parallelism.
  Effective resource management.


1 Resource Management

1.1 Persistent Objects

Best Practice: Create persistent objects early and reuse them often.

The Metal framework provides several protocols to manage persistent objects throughout the lifetime of your app. These objects are expensive to create but are usually initialized once and reused often. Do not create these objects at the beginning of every render or compute loop.


  Initialize Your Device and Command Queue First
  Compile Your Functions and Build Your Library at Build Time
  Build Your Pipelines Once and Reuse Them Often
  Allocate Resource Storage Up Front


For more information, consult the Persistent Objects section of the documentation.

1.2 Resource Options

Best Practice: Set appropriate resource storage modes and texture usage options for your resources.

Metal resources must be configured appropriately to take advantage of fast memory access and driver performance optimizations. Resource storage modes allow you to define the storage location and access permissions for your MTLBuffer and MTLTexture objects. Texture usage options allow you to explicitly declare how you intend to use your MTLTexture objects.


  Familiarize Yourself with Device Memory Models
  Choose an Appropriate Resource Storage Mode (iOS and tvOS)
  Choose an Appropriate Resource Storage Mode (OS X)
  Set Appropriate Texture Usage Flags


For more information, consult the Resource Options section of the documentation.

1.3 Triple Buffering

Best Practice: Implement a triple buffering model to update dynamic buffer data.

Dynamic buffer data refers to frequently-updated data stored in a buffer. To avoid creating new buffers per frame and to minimize processor idle time between frames, implementing a triple buffering model is strongly recommended.


  Prevent Access Conflicts and Reduce Processor Idle Time
  Reduce Memory Overhead and Frame Latency
  Allow Time for Command Buffer Transactions
  Implement a Triple Buffering Model


For more information, consult the Triple Buffering section of the documentation.

1.4 Buffer Bindings

Best Practice: Use an appropriate method to bind your buffer data to a graphics or compute function.

Metal provides several API options for binding buffer data to a graphics or compute function. The setVertexBytes:length:atIndex: method is the best option for binding an amount of dynamic buffer data (a transient buffer) that is less than 4 KB to a vertex function. If the data size is larger than 4 KB, you should create a MTLBuffer once and update its contents as needed.

For more information, consult the Buffer Bindings section of the documentation.

2 Display Management

2.1 Drawables

Best Practice: Hold a drawable as briefly as possible.

The command buffer is used to schedule a drawable’s presentation with the presentDrawable: method before the command buffer itself is scheduled for execution, however, the drawable itself is actually presented after the command buffer has completed execution.


  Use a MetalKit View to Acquire a Drawable


For more information, consult the Drawables section of the documentation.

2.2 Native Screen Scale (iOS and tvOS)

Best Practice: Render at the exact pixel size of your target display.

The pixel size of your drawables should always match the exact pixel size of their target display. This is critical to avoid rendering to off-screen pixels or incurring an additional sampling stage.


  Use a MetalKit View to Support Native Screen Scale


For more information, consult the Native Screen Scale section of the documentation.

2.3 Frame Rate (iOS and tvOS)

Best Practice: For apps that can’t maintain a 60 FPS frame rate, present your drawables at a steady frame rate.

The display refresh rate of iOS devices is 60 Hz. Apps that are consistently unable to complete a frame’s work within this time should target a lower frame rate to avoid jitter. The display refresh rate of tvOS devices is usually, but not always, 60 Hz.


  Use the Display Link
  Adjust the Frame Interval
  Adjust the Drawable Presentation Time


For more information, consult the Frame Rate section of the documentation.

3 Command Generation

3.1 Load and Store Actions

Best Practice: Set appropriate load and store actions for your render targets.

Actions performed on your Metal render targets must be configured appropriately to avoid costly and unnecessary rendering work at the start (load action) or end (store action) of a rendering pass.


  Choose an Appropriate Load Action
  Choose an Appropriate Store Action
  Evaluate Actions Between Rendering Passes


For more information, consult the Load and Store Actions section of the documentation.

3.2 Render Command Encoders (iOS and tvOS)

Best Practice: Merge render command encoders when possible.

Eliminating unnecessary render command encoders reduces memory bandwidth and increases performance.


  Evaluate Rendering Pass Order
  Evaluate Sampling Dependencies
  Evaluate Actions Between Rendering Passes


For more information, consult the Render Command Encoders section of the documentation.

3.3 Command Buffers

Best Practice: Submit the fewest command buffers per frame without underutilizing the GPU.

Command buffers are the unit of work submission in Metal; they are created by the CPU and executed by the GPU. This relationship allows you to balance CPU and GPU work by adjusting the number of command buffers submitted per frame.

For more information, consult the Command Buffers section of the documentation.

3.4 Indirect Buffers

Best Practice: Use indirect buffers if your draw or dispatch call arguments are dynamically generated by the GPU.

Indirect buffers are MTLBuffer objects with a specific data layout representing draw or dispatch call arguments.


  Eliminate Unnecessary Data Transfers and Reduce Processor Idle Time


For more information, consult the Indirect Buffers section of the documentation.

4 Compilation

4.1 Functions and Libraries

Best Practice: Compile your functions and build your library at build time.

Compiling Metal Shading Language source code is one of the most expensive stages in a Metal app. Metal is designed to minimize this cost by allowing you to compile graphics and compute functions at build time, then load them at runtime as a library.


  Build Your Library at Build Time
  Group Your Functions into a Single Library


For more information, consult the Functions and Libraries section of the documentation.

4.2 Pipelines

Best Practice: Build your render and compute pipelines asynchronously.

Having multiple render or compute pipelines allows your app to use different state configurations for specific tasks. Building these pipelines asynchronously maximizes performance and parallelism. It is recommended that you build all known pipelines up front and avoid lazy loading.

For more information, consult the Pipelines section of the documentation.

This guide, along with the Metal Programming Guide and the Metal Shading Language Guide both already updated for iOS 10, tvOS 10 and OS X 10.12, give you the trilogy of documents containing everything you need to start creating performant Metal apps.

Until next time!</summary></entry><entry><title type="html">Using MetalKit part 15</title><link href="http://localhost:4000/2016/06/23/using-metalkit-part-15.html" rel="alternate" type="text/html" title="Using MetalKit part 15" /><published>2016-06-23T00:00:00+03:00</published><updated>2016-06-23T00:00:00+03:00</updated><id>http://localhost:4000/2016/06/23/using-metalkit-part-15</id><content type="html" xml:base="http://localhost:4000/2016/06/23/using-metalkit-part-15.html">At the end of `part 13` we concluded we can make our planet look more realistic in two ways: either apply a texture to it, or add some noise to the `planet` color. We showed how to add noise in `part 14`. This week we look at __textures__ and __samplers__. Textures are useful because they can provide a greater level of detail to surfaces than color computing for each vertex could.

Let's pick up where we left off in [Part 13](http://metalkit.org/2016/05/25/using-metalkit-part-13.html) since we do not need the noise code this time. First, in `MetalView.swift` let's remove the `mouseDown` function as we are not going to need it anymore. Also remove the `mouseBuffer` and `pos` variables, as well as any references to them in the code. Then, create a new texture object:

{% highlight swift %}var texture: MTLTexture!
{% endhighlight %}

Next, replace this line (it's likely you removed it already in the above cleaning step):

{% highlight swift %}commandEncoder.setBuffer(mouseBuffer, offset: 0, atIndex: 2)
{% endhighlight %}

with this line:

{% highlight swift %}commandEncoder.setTexture(texture, atIndex: 1)
{% endhighlight %}

and also change the buffer index for our `timer` from __1__ to __0__:

{% highlight swift %}commandEncoder.setBuffer(timerBuffer, offset: 0, atIndex: 0)
{% endhighlight %}

I added an image named __texture.jpg__ in the `Resources` folder of our playground, but you can add yours instead if you want. Let's create a function that sets up or texture using this image:

{% highlight swift %}func setUpTexture() {
    let path = NSBundle.mainBundle().pathForResource(&quot;texture&quot;, ofType: &quot;jpg&quot;)
    let textureLoader = MTKTextureLoader(device: device!)
    texture = try! textureLoader.newTextureWithContentsOfURL(NSURL(fileURLWithPath: path!), options: nil)
}
{% endhighlight %}

Next, call this function in our `init` function:

{% highlight swift %}override public init(frame frameRect: CGRect, device: MTLDevice?) {
    super.init(frame: frameRect, device: device)
    registerShaders()
    setUpTexture()
}
{% endhighlight %}

Now, let's clean our kernel in __Shaders.metal__ to only include these lines:

{% highlight swift %}kernel void compute(texture2d&lt;float, access::write&gt; output [[texture(0)]],
                    texture2d&lt;float, access::read&gt; input [[texture(1)]],
                    constant float &amp;timer [[buffer(1)]],
                    uint2 gid [[thread_position_in_grid]])
{
    float4 color = input.read(gid);
    gid.y = input.get_height() - gid.y;
    output.write(color, gid);
}
{% endhighlight %}

You will first notice that we get the `input` texture through the __[[texture(1)]]__ attribute since that is the index we set it to in the command encoder. Also, the access we requested for it is __read__. Then we read it into the `color` variable, however, it comes in flipped upside-down. In order to fix this, on the next line we just flip the __Y__ coordinate for each pixel. The output image should look like this:

![alt text](https://github.com/MetalKit/images/raw/master/chapter15_1.png &quot;1&quot;)

If you open the image and compare it with the output, you will notice it is now correctly oriented. Next, we want to bring back our planet and the dark sky around it. Replace the `output` line with this block of code:

{% highlight swift %}int width = input.get_width();
int height = input.get_height();
float2 uv = float2(gid) / float2(width, height);
uv = uv * 2.0 - 1.0;
float radius = 0.5;
float distance = length(uv) - radius;
output.write(distance &lt; 0 ? color : float4(0), gid);
{% endhighlight %}

This code looks familiar since we already discussed in the previous chapters how to create the planet and the black space surrounding it. The output image should look like this:

![alt text](https://github.com/MetalKit/images/raw/master/chapter15_2.png &quot;2&quot;)

So far so good! We next want to make our planet rotate again. Replace the `output` line with this block of code:

{% highlight swift %}uv = fmod(float2(gid) + float2(timer * 100, 0), float2(width, height));
color = input.read(uint2(uv));
output.write(distance &lt; 0 ? color : float4(0), gid);
{% endhighlight %}

This code again looks familiar from the previous chapters where we discussed how to use `timer` to animate the planet. The output image should look like this:

![alt text](https://github.com/MetalKit/images/raw/master/chapter15_3.gif &quot;3&quot;)

This is a bit awkward! The output looks like someone would walk in a dark cave, next to the wall and carrying a torch. Replace the last three lines we added with this block of code:

{% highlight swift %}uv = uv * 2;
radius = 1;
constexpr sampler textureSampler(coord::normalized,
                                 address::repeat,
                                 min_filter::linear,
                                 mag_filter::linear,
                                 mip_filter::linear );
float3 norm = float3(uv, sqrt(1.0 - dot(uv, uv)));
float pi = 3.14;
float s = atan2( norm.z, norm.x ) / (2 * pi);
float t = asin( norm.y ) / (2 * pi);
t += 0.5;
color = input.sample(textureSampler, float2(s + timer * 0.1, t));
output.write(distance &lt; 0 ? color : float4(0), gid);
{% endhighlight %}

First, we  scale down to half the size of the texture and set the radius to __1__ so we can match the planet object size with the texture size. Then comes the magic. Let me introduce the __sampler__. A `sampler` is an object that contains various rendering states that a texture needs to configure: its coordinates, the addressing mode (set to `repeat` here) and the filtering method (set to `linear` here). Next, we calculate the `normal` at each point on the sphere, then we compute the angles around the sphere using the normals. Finally, we calculate the `color` by sampling it instead of reading it as we did before. There is one more thing to do. In the kernel list of arguments, let's also reconfigure the texture access to `sample` instead of `read`. Replace this line:
 
{% highlight swift %}texture2d&lt;float, access::read&gt; input [[texture(1)]],
{% endhighlight %}
 
with this line:

{% highlight swift %}texture2d&lt;float, access::sample&gt; input [[texture(1)]],
{% endhighlight %}

The output image should look like this:

![alt text](https://github.com/MetalKit/images/raw/master/chapter15_4.gif &quot;4&quot;)

Now this is what I call a realistic planet surface! Many thanks again to [Chris](https://twitter.com/_psonice) for his assistance. The [source code](https://github.com/MetalKit/metal) is posted on Github as usual.

Until next time!</content><summary type="html">At the end of part 13 we concluded we can make our planet look more realistic in two ways: either apply a texture to it, or add some noise to the planet color. We showed how to add noise in part 14. This week we look at textures and samplers. Textures are useful because they can provide a greater level of detail to surfaces than color computing for each vertex could.

Let’s pick up where we left off in Part 13 since we do not need the noise code this time. First, in MetalView.swift let’s remove the mouseDown function as we are not going to need it anymore. Also remove the mouseBuffer and pos variables, as well as any references to them in the code. Then, create a new texture object:

var texture: MTLTexture!

Next, replace this line (it’s likely you removed it already in the above cleaning step):

commandEncoder.setBuffer(mouseBuffer, offset: 0, atIndex: 2)

with this line:

commandEncoder.setTexture(texture, atIndex: 1)

and also change the buffer index for our timer from 1 to 0:

commandEncoder.setBuffer(timerBuffer, offset: 0, atIndex: 0)

I added an image named texture.jpg in the Resources folder of our playground, but you can add yours instead if you want. Let’s create a function that sets up or texture using this image:

func setUpTexture() {
    let path = NSBundle.mainBundle().pathForResource(&quot;texture&quot;, ofType: &quot;jpg&quot;)
    let textureLoader = MTKTextureLoader(device: device!)
    texture = try! textureLoader.newTextureWithContentsOfURL(NSURL(fileURLWithPath: path!), options: nil)
}

Next, call this function in our init function:

override public init(frame frameRect: CGRect, device: MTLDevice?) {
    super.init(frame: frameRect, device: device)
    registerShaders()
    setUpTexture()
}

Now, let’s clean our kernel in Shaders.metal to only include these lines:

kernel void compute(texture2d&amp;lt;float, access::write&amp;gt; output [[texture(0)]],
                    texture2d&amp;lt;float, access::read&amp;gt; input [[texture(1)]],
                    constant float &amp;amp;timer [[buffer(1)]],
                    uint2 gid [[thread_position_in_grid]])
{
    float4 color = input.read(gid);
    gid.y = input.get_height() - gid.y;
    output.write(color, gid);
}

You will first notice that we get the input texture through the [[texture(1)]] attribute since that is the index we set it to in the command encoder. Also, the access we requested for it is read. Then we read it into the color variable, however, it comes in flipped upside-down. In order to fix this, on the next line we just flip the Y coordinate for each pixel. The output image should look like this:



If you open the image and compare it with the output, you will notice it is now correctly oriented. Next, we want to bring back our planet and the dark sky around it. Replace the output line with this block of code:

int width = input.get_width();
int height = input.get_height();
float2 uv = float2(gid) / float2(width, height);
uv = uv * 2.0 - 1.0;
float radius = 0.5;
float distance = length(uv) - radius;
output.write(distance &amp;lt; 0 ? color : float4(0), gid);

This code looks familiar since we already discussed in the previous chapters how to create the planet and the black space surrounding it. The output image should look like this:



So far so good! We next want to make our planet rotate again. Replace the output line with this block of code:

uv = fmod(float2(gid) + float2(timer * 100, 0), float2(width, height));
color = input.read(uint2(uv));
output.write(distance &amp;lt; 0 ? color : float4(0), gid);

This code again looks familiar from the previous chapters where we discussed how to use timer to animate the planet. The output image should look like this:



This is a bit awkward! The output looks like someone would walk in a dark cave, next to the wall and carrying a torch. Replace the last three lines we added with this block of code:

uv = uv * 2;
radius = 1;
constexpr sampler textureSampler(coord::normalized,
                                 address::repeat,
                                 min_filter::linear,
                                 mag_filter::linear,
                                 mip_filter::linear );
float3 norm = float3(uv, sqrt(1.0 - dot(uv, uv)));
float pi = 3.14;
float s = atan2( norm.z, norm.x ) / (2 * pi);
float t = asin( norm.y ) / (2 * pi);
t += 0.5;
color = input.sample(textureSampler, float2(s + timer * 0.1, t));
output.write(distance &amp;lt; 0 ? color : float4(0), gid);

First, we  scale down to half the size of the texture and set the radius to 1 so we can match the planet object size with the texture size. Then comes the magic. Let me introduce the sampler. A sampler is an object that contains various rendering states that a texture needs to configure: its coordinates, the addressing mode (set to repeat here) and the filtering method (set to linear here). Next, we calculate the normal at each point on the sphere, then we compute the angles around the sphere using the normals. Finally, we calculate the color by sampling it instead of reading it as we did before. There is one more thing to do. In the kernel list of arguments, let’s also reconfigure the texture access to sample instead of read. Replace this line:

texture2d&amp;lt;float, access::read&amp;gt; input [[texture(1)]],

with this line:

texture2d&amp;lt;float, access::sample&amp;gt; input [[texture(1)]],

The output image should look like this:



Now this is what I call a realistic planet surface! Many thanks again to Chris for his assistance. The source code is posted on Github as usual.

Until next time!</summary></entry><entry><title type="html">What’s new in graphics and games at WWDC 2016</title><link href="http://localhost:4000/2016/06/14/what-s-new-in-graphics-and-games-at-wwdc-2016.html" rel="alternate" type="text/html" title="What's new in graphics and games at WWDC 2016" /><published>2016-06-14T00:00:00+03:00</published><updated>2016-06-14T00:00:00+03:00</updated><id>http://localhost:4000/2016/06/14/what-s-new-in-graphics-and-games-at-wwdc-2016</id><content type="html" xml:base="http://localhost:4000/2016/06/14/what-s-new-in-graphics-and-games-at-wwdc-2016.html">Like every year, `June` is my favorite month of the year for several reasons, but __WWDC__ is most likely the top one! Watching the opening `Keynote` and the `Platforms State Of The Union` sessions yesterday revealed a plethora of new features and even a few new frameworks. In this article, I am only going to focus on what's new in the __Graphics and Games__ track.

Let's start with __Metal__, obviously. By far, the hottest and most anticipated feature is support for __Tessellation__ which enables 3D apps and games to render more details by efficiently describing complex geometry to the `GPU`. Another feature is __Function Specialization__ which helps with creating a collection of functions particularly optimized to handle material and light combinations in a scene. Also new this year are the __Resource Heaps and Memoryless Render Targets__ for finer-grained control of resource allocation and performance optimization in `iOS` and `tvOS`. Finally, the __Metal System Trace__ is a `macOS`-only feature that helps us analyzing the graphics pipeline by profiling the interaction between the `CPU` and the `GPU`, thus helping us finding performance optimization points for Metal-based apps.

The __Model I/O__ framework brings support for the __USD__ file format. The __MDLMaterialPropertyGraph__ class now makes it easier to support runtime procedural changes to models. Also, the __MDLVoxelArray__ class now adds support for signed distance fields. Finally, you can now add assisted light probe placement through the __MDLLightProbeIrradianceDataSource__ protocol.

The __GameplayKit__ framework brings us __Procedural noise generation__ that can be used to generate richer game worlds, more sophisticated textures, more realistic to camera movements. Next,
__Spatial partitioning__ lets us partition game world data so that it can be searched efficiently. Also, the new __Monte Carlo strategist__ helps us model games where exhaustive computation of possible moves is difficult. The new __decision tree API__ can enhance our game-building AI when adopting decision-tree learning to generalize behavior based on data mining of logged player actions. 

The __ReplayKit__ framework introduces support for __tvOS__ and for __broadcasting__ so we can broadcast recorded media through a third-party site. 

The __SceneKit__ framework introduces a new __Physically Based Rendering__ system that empowers us to create more realistic results with simpler asset authoring. Also, the new HDR features and effects help us creating even more realism.

The __SpriteKit__ framework introduces a new tilemap solution to support square, hexagonal, and isometric tilemaps. The Xcode editor also provides support for organizing the tiles and the tilemap.

The __Accelerate__ framework introduces support for `quadrature` (integral calculus), basic functions for constructing neural networks, and geometric predicate functions to test for object intersections.

The __Core Image__ framework now allows us to insert custom processing into a `Core Image` filter graph. `Core Image` kernel code can now request a specific output pixel format. Finally, `Core Image` adds five new filters to the existing filter collection.

Stay tuned for more news, and have a great `WWDC`!</content><summary type="html">Like every year, June is my favorite month of the year for several reasons, but WWDC is most likely the top one! Watching the opening Keynote and the Platforms State Of The Union sessions yesterday revealed a plethora of new features and even a few new frameworks. In this article, I am only going to focus on what’s new in the Graphics and Games track.

Let’s start with Metal, obviously. By far, the hottest and most anticipated feature is support for Tessellation which enables 3D apps and games to render more details by efficiently describing complex geometry to the GPU. Another feature is Function Specialization which helps with creating a collection of functions particularly optimized to handle material and light combinations in a scene. Also new this year are the Resource Heaps and Memoryless Render Targets for finer-grained control of resource allocation and performance optimization in iOS and tvOS. Finally, the Metal System Trace is a macOS-only feature that helps us analyzing the graphics pipeline by profiling the interaction between the CPU and the GPU, thus helping us finding performance optimization points for Metal-based apps.

The Model I/O framework brings support for the USD file format. The MDLMaterialPropertyGraph class now makes it easier to support runtime procedural changes to models. Also, the MDLVoxelArray class now adds support for signed distance fields. Finally, you can now add assisted light probe placement through the MDLLightProbeIrradianceDataSource protocol.

The GameplayKit framework brings us Procedural noise generation that can be used to generate richer game worlds, more sophisticated textures, more realistic to camera movements. Next,
Spatial partitioning lets us partition game world data so that it can be searched efficiently. Also, the new Monte Carlo strategist helps us model games where exhaustive computation of possible moves is difficult. The new decision tree API can enhance our game-building AI when adopting decision-tree learning to generalize behavior based on data mining of logged player actions.

The ReplayKit framework introduces support for tvOS and for broadcasting so we can broadcast recorded media through a third-party site.

The SceneKit framework introduces a new Physically Based Rendering system that empowers us to create more realistic results with simpler asset authoring. Also, the new HDR features and effects help us creating even more realism.

The SpriteKit framework introduces a new tilemap solution to support square, hexagonal, and isometric tilemaps. The Xcode editor also provides support for organizing the tiles and the tilemap.

The Accelerate framework introduces support for quadrature (integral calculus), basic functions for constructing neural networks, and geometric predicate functions to test for object intersections.

The Core Image framework now allows us to insert custom processing into a Core Image filter graph. Core Image kernel code can now request a specific output pixel format. Finally, Core Image adds five new filters to the existing filter collection.

Stay tuned for more news, and have a great WWDC!</summary></entry><entry><title type="html">Using MetalKit part 14</title><link href="http://localhost:4000/2016/06/01/using-metalkit-part-14.html" rel="alternate" type="text/html" title="Using MetalKit part 14" /><published>2016-06-01T00:00:00+03:00</published><updated>2016-06-01T00:00:00+03:00</updated><id>http://localhost:4000/2016/06/01/using-metalkit-part-14</id><content type="html" xml:base="http://localhost:4000/2016/06/01/using-metalkit-part-14.html">Let's pick up where we left off in [Part 13](http://metalkit.org/2016/05/25/using-metalkit-part-13.html). Using the same playground we worked on last time, we will learn about __noise__ today. From _Wikipedia_:

&gt; `Noise` refers to any random fluctuations of data that makes the perception of an expected signal, difficult. `Value noise` is a type of noise commonly used as a procedural texture primitive in computer graphics. This method consists of the creation of a lattice of points which are assigned random values. The noise function then returns the interpolated number based on the values of the surrounding lattice points. Multiple octaves of this noise can be generated and then summed together in order to create a form of fractal noise.

The most obvious characteristic of noise is randomness. Since `MSL` does not provide a random function let's just create one ourselves. What we need is a random number between __[0, 1]__. We can get that by using the `fract` function which returns the fractional component of a number:

{% highlight swift %}float random(float2 p)
{
    return fract(sin(dot(p, float2(15.79, 81.93)) * 45678.9123));
}
{% endhighlight %}

The __noise()__ function will bilinearly interpolate a lattice (grid) and return a smoothed value. Bilinear interpolation allows us to transform our __1D__ `random` function to a value based on a __2D__ grid:

{% highlight swift %}float noise(float2 p)
{
    float2 i = floor(p);
    float2 f = fract(p);
    f = f * f * (3.0 - 2.0 * f);
    float bottom = mix(random(i + float2(0)), random(i + float2(1.0, 0.0)), f.x);
    float top = mix(random(i + float2(0.0, 1.0)), random(i + float2(1)), f.x);
    float t = mix(bottom, top, f.y);
    return t;
}
{% endhighlight %}

We first use __i__ to move along grid points and __f__ as an offset between the grid points. Then we calculate a __Cubic Hermite Spline__ with the formula `3f^2 - 2f^3` and which creates a S-shaped curve that has values between __[0, 1]__. Next we interpolate values along the bottom and top of the grid, and finally we interpolate the vertical line between those 2 horizontal points to get our final value for noise.

Next we create a __Fractional Brownian Motion__ function that calls our `noise()` function multiple times and adds up the results. 

{% highlight swift %}float fbm(float2 uv)
{
    float sum = 0;
    float amp = 0.7;
    for(int i = 0; i &lt; 4; ++i)
    {
        sum += noise(uv) * amp;
        uv += uv * 1.2;
        amp *= 0.4;
    }
    return sum;
}
{% endhighlight %}

By adding various (four -- in this case) `octaves` of this noise at different amplitudes (as explained in the beginning), we can generate a simple cloud-like pattern. We have one more thing to do: inside the kernel replace all the lines after the one where `distance` is defined, with these lines:

{% highlight swift %}uv = fmod(uv + float2(timer * 0.2, 0), float2(width, height));
float t = fbm( uv * 3 );
output.write(distance &lt; 0 ? float4(float3(t), 1) : float4(0), gid);
{% endhighlight %}

For fun, we add the `timer` uniform again to animate the content. The output image should look like this:

![alt text](https://github.com/MetalKit/images/raw/master/chapter14.gif &quot;chapter 14&quot;)

It looks cool, but still not realistic enough. To make it look even more realistic we need to learn about, and apply a texture. You can read more about [bilinear filtering](http://www.scratchapixel.com/old/lessons/3d-advanced-lessons/interpolation/bilinear-interpolation), about [value noise](http://www.scratchapixel.com/old/lessons/3d-advanced-lessons/noise-part-1/creating-a-simple-2d-noise) and about [Fractional Brownian motion](https://en.wikipedia.org/wiki/Fractional_Brownian_motion) if you're interested. You can also see an example of the [Cubic Hermit Spline](https://www.desmos.com/calculator/mnrgw3yias). The [source code](https://github.com/MetalKit/metal) is posted on Github as usual.

Until next time!</content><summary type="html">Let’s pick up where we left off in Part 13. Using the same playground we worked on last time, we will learn about noise today. From Wikipedia:


  Noise refers to any random fluctuations of data that makes the perception of an expected signal, difficult. Value noise is a type of noise commonly used as a procedural texture primitive in computer graphics. This method consists of the creation of a lattice of points which are assigned random values. The noise function then returns the interpolated number based on the values of the surrounding lattice points. Multiple octaves of this noise can be generated and then summed together in order to create a form of fractal noise.


The most obvious characteristic of noise is randomness. Since MSL does not provide a random function let’s just create one ourselves. What we need is a random number between [0, 1]. We can get that by using the fract function which returns the fractional component of a number:

float random(float2 p)
{
    return fract(sin(dot(p, float2(15.79, 81.93)) * 45678.9123));
}

The noise() function will bilinearly interpolate a lattice (grid) and return a smoothed value. Bilinear interpolation allows us to transform our 1D random function to a value based on a 2D grid:

float noise(float2 p)
{
    float2 i = floor(p);
    float2 f = fract(p);
    f = f * f * (3.0 - 2.0 * f);
    float bottom = mix(random(i + float2(0)), random(i + float2(1.0, 0.0)), f.x);
    float top = mix(random(i + float2(0.0, 1.0)), random(i + float2(1)), f.x);
    float t = mix(bottom, top, f.y);
    return t;
}

We first use i to move along grid points and f as an offset between the grid points. Then we calculate a Cubic Hermite Spline with the formula 3f^2 - 2f^3 and which creates a S-shaped curve that has values between [0, 1]. Next we interpolate values along the bottom and top of the grid, and finally we interpolate the vertical line between those 2 horizontal points to get our final value for noise.

Next we create a Fractional Brownian Motion function that calls our noise() function multiple times and adds up the results.

float fbm(float2 uv)
{
    float sum = 0;
    float amp = 0.7;
    for(int i = 0; i &amp;lt; 4; ++i)
    {
        sum += noise(uv) * amp;
        uv += uv * 1.2;
        amp *= 0.4;
    }
    return sum;
}

By adding various (four – in this case) octaves of this noise at different amplitudes (as explained in the beginning), we can generate a simple cloud-like pattern. We have one more thing to do: inside the kernel replace all the lines after the one where distance is defined, with these lines:

uv = fmod(uv + float2(timer * 0.2, 0), float2(width, height));
float t = fbm( uv * 3 );
output.write(distance &amp;lt; 0 ? float4(float3(t), 1) : float4(0), gid);

For fun, we add the timer uniform again to animate the content. The output image should look like this:



It looks cool, but still not realistic enough. To make it look even more realistic we need to learn about, and apply a texture. You can read more about bilinear filtering, about value noise and about Fractional Brownian motion if you’re interested. You can also see an example of the Cubic Hermit Spline. The source code is posted on Github as usual.

Until next time!</summary></entry><entry><title type="html">Using MetalKit part 13</title><link href="http://localhost:4000/2016/05/25/using-metalkit-part-13.html" rel="alternate" type="text/html" title="Using MetalKit part 13" /><published>2016-05-25T00:00:00+03:00</published><updated>2016-05-25T00:00:00+03:00</updated><id>http://localhost:4000/2016/05/25/using-metalkit-part-13</id><content type="html" xml:base="http://localhost:4000/2016/05/25/using-metalkit-part-13.html">Let's pick up where we left off in [Part 12](http://metalkit.org/2016/05/18/using-metalkit-part-12.html). Using the same playground we worked on last time, we will learn about lighting and `3D` objects today. Remember the sun eclipse we worked on a couple of weeks ago? It's back! Well, we are going to remove the sun and just focus on the planet this time.

First, let's clean our kernel to only include this code:

{% highlight swift %}int width = output.get_width();
int height = output.get_height();
float2 uv = float2(gid) / float2(width, height);
uv = uv * 2.0 - 1.0;
float radius = 0.5;
float distance = length(uv) - radius;
output.write(distance &lt; 0 ? float4(1) : float4(0), gid);
{% endhighlight %}

You surely recognize all this code from few weeks ago. The only thing we did is replace the outside circle color with `black` and the inside circle color with `white`. The output image should look like this:

![alt text](https://github.com/MetalKit/images/raw/master/chapter13_0.png &quot;0&quot;)

So far so good. The planet looks pretty flat and the lighting is too uniformly distributed to look real. Let's fix that next. Geometry tells us that in order to find any point on a sphere, we need the sphere equation:

![alt text](https://github.com/MetalKit/images/raw/master/chapter13_2.png &quot;2&quot;)

In our particular case __x0__, __y0__ and __z0__ are all __0__ because our sphere is in the center of the screen. Solving for __z__ gives us the the value of the `planet` color, so let's replace the last line in the kernel, with these lines:

{% highlight swift %}float planet = float(sqrt(radius * radius - uv.x * uv.x - uv.y * uv.y));
planet /= radius;
output.write(distance &lt; 0 ? float4(planet) : float4(0), gid);
{% endhighlight %}

The output image should look like this:

![alt text](https://github.com/MetalKit/images/raw/master/chapter13_1.png &quot;1&quot;)

As you expected, the color is now calculated starting with fully white in the center of the circle and ending  with fully black on the outer circle. For that to happen, we had to divide the color by the `radius`, in order to normalize our range to the __[0, 1]__ interval for the `z` value, which gives us a full range light effect. We actually faked having a light source positioned at __(0, 0, 1)__. This brings up to the next topic: `lighting`.

__Lighting__ is what gives life to our colors. In order to have lights in our scene we need to compute the `normal` at each coordinate. Normals vectors that are are perpendicular on the surface, showing us where the surface &quot;points&quot; to at each coordinate. Replace the last two lines with these lines:

{% highlight swift %}float3 normal = normalize(float3(uv.x, uv.y, planet));
output.write(distance &lt; 0 ? float4(float3(normal), 1) : float4(0), gid);
{% endhighlight %}

Notice we already had the value of `z` in the `planet` variable. The output image should look like this:

![alt text](https://github.com/MetalKit/images/raw/master/chapter13_5.png &quot;5&quot;)

This is probably not what we wanted to see, but at least we now know how normals look like when calculating the color at each normalized coordinate. Next, let's create a source of light located to our left (negative `x`) and a little behind us (positive `z`). Replace the last line with these lines:

{% highlight swift %}float3 source = normalize(float3(-1, 0, 1));
float light = dot(normal, source);
output.write(distance &lt; 0 ? float4(float3(light), 1) : float4(0), gid); 
{% endhighlight %}

We adopted a basic light model called [Lambertian](https://en.wikipedia.org/wiki/Lambertian_reflectance) (diffuse) light, where we need to multiply the normal with the normalized light source. We will talk more about lighting in a future article, however, if you are interested in learning more about lighting models, [here](https://www.evl.uic.edu/aej/488/lecture12.html) is a great resource for your reference. The output image should look like this:

![alt text](https://github.com/MetalKit/images/raw/master/chapter13_3.png &quot;3&quot;)

Remember from last time that our kernel also gives us a timer uniform? Let's use it for fun and profit! Replace the `source` line with this one:

{% highlight swift %}float3 source = normalize(float3(cos(timer), sin(timer), 1));
{% endhighlight %}

By using the `cos` and `sin` functions, we gave the light source a circular movement. `x` and `y` are both ranging from __-1__ to __1__ using the parametric equation of a circle. The output image should look like this:

![alt text](https://github.com/MetalKit/images/raw/master/chapter13_6.gif &quot;6&quot;)

We how have a good looking, illuminated object in the scene (planet in the sky), however, the object still presents a homogeneous surface. We can make it look more realistic in two ways: either apply a texture to it, or add some noise to the `planet` color. The [source code](https://github.com/MetalKit/metal) is posted on Github as usual.

Until next time!</content><summary type="html">Let’s pick up where we left off in Part 12. Using the same playground we worked on last time, we will learn about lighting and 3D objects today. Remember the sun eclipse we worked on a couple of weeks ago? It’s back! Well, we are going to remove the sun and just focus on the planet this time.

First, let’s clean our kernel to only include this code:

int width = output.get_width();
int height = output.get_height();
float2 uv = float2(gid) / float2(width, height);
uv = uv * 2.0 - 1.0;
float radius = 0.5;
float distance = length(uv) - radius;
output.write(distance &amp;lt; 0 ? float4(1) : float4(0), gid);

You surely recognize all this code from few weeks ago. The only thing we did is replace the outside circle color with black and the inside circle color with white. The output image should look like this:



So far so good. The planet looks pretty flat and the lighting is too uniformly distributed to look real. Let’s fix that next. Geometry tells us that in order to find any point on a sphere, we need the sphere equation:



In our particular case x0, y0 and z0 are all 0 because our sphere is in the center of the screen. Solving for z gives us the the value of the planet color, so let’s replace the last line in the kernel, with these lines:

float planet = float(sqrt(radius * radius - uv.x * uv.x - uv.y * uv.y));
planet /= radius;
output.write(distance &amp;lt; 0 ? float4(planet) : float4(0), gid);

The output image should look like this:



As you expected, the color is now calculated starting with fully white in the center of the circle and ending  with fully black on the outer circle. For that to happen, we had to divide the color by the radius, in order to normalize our range to the [0, 1] interval for the z value, which gives us a full range light effect. We actually faked having a light source positioned at (0, 0, 1). This brings up to the next topic: lighting.

Lighting is what gives life to our colors. In order to have lights in our scene we need to compute the normal at each coordinate. Normals vectors that are are perpendicular on the surface, showing us where the surface “points” to at each coordinate. Replace the last two lines with these lines:

float3 normal = normalize(float3(uv.x, uv.y, planet));
output.write(distance &amp;lt; 0 ? float4(float3(normal), 1) : float4(0), gid);

Notice we already had the value of z in the planet variable. The output image should look like this:



This is probably not what we wanted to see, but at least we now know how normals look like when calculating the color at each normalized coordinate. Next, let’s create a source of light located to our left (negative x) and a little behind us (positive z). Replace the last line with these lines:

float3 source = normalize(float3(-1, 0, 1));
float light = dot(normal, source);
output.write(distance &amp;lt; 0 ? float4(float3(light), 1) : float4(0), gid); 

We adopted a basic light model called Lambertian (diffuse) light, where we need to multiply the normal with the normalized light source. We will talk more about lighting in a future article, however, if you are interested in learning more about lighting models, here is a great resource for your reference. The output image should look like this:



Remember from last time that our kernel also gives us a timer uniform? Let’s use it for fun and profit! Replace the source line with this one:

float3 source = normalize(float3(cos(timer), sin(timer), 1));

By using the cos and sin functions, we gave the light source a circular movement. x and y are both ranging from -1 to 1 using the parametric equation of a circle. The output image should look like this:



We how have a good looking, illuminated object in the scene (planet in the sky), however, the object still presents a homogeneous surface. We can make it look more realistic in two ways: either apply a texture to it, or add some noise to the planet color. The source code is posted on Github as usual.

Until next time!</summary></entry></feed>
