<?xml version="1.0" encoding="utf-8"?><?xml-stylesheet type="text/xml" href="http://localhost:4000/feed.xslt.xml"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="http://jekyllrb.com" version="3.3.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2017-12-01T00:44:42-06:00</updated><id>http://localhost:4000//</id><title type="html">The Metal Framework</title><subtitle>Resources and tutorials for Metal, MetalKit and Metal Performance Shaders.
</subtitle><entry><title type="html">Working with Particles in Metal part 3</title><link href="http://localhost:4000/2017/11/30/working-with-particles-in-metal-part-3.html" rel="alternate" type="text/html" title="Working with Particles in Metal part 3" /><published>2017-11-30T00:00:00-06:00</published><updated>2017-11-30T00:00:00-06:00</updated><id>http://localhost:4000/2017/11/30/working-with-particles-in-metal-part-3</id><content type="html" xml:base="http://localhost:4000/2017/11/30/working-with-particles-in-metal-part-3.html">&lt;p&gt;Last time we looked at how to manipulate vertices from &lt;code class=&quot;highlighter-rouge&quot;&gt;Model I/O&lt;/code&gt; objects on the &lt;code class=&quot;highlighter-rouge&quot;&gt;GPU&lt;/code&gt;. In this part we are going to show yet another way to create particles using compute threads. We can reuse the playground from last time and we start by modifying the &lt;strong&gt;Particle&lt;/strong&gt; struct in our metal view delegate class to only include two members that we will update on the &lt;code class=&quot;highlighter-rouge&quot;&gt;GPU&lt;/code&gt; - &lt;strong&gt;position&lt;/strong&gt; and &lt;strong&gt;velocity&lt;/strong&gt;:&lt;/p&gt;

&lt;div class=&quot;language-swift highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kd&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Particle&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;position&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;float2&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;velocity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;float2&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;We need neither the &lt;strong&gt;timer&lt;/strong&gt; variable, nor the &lt;strong&gt;translate(by:)&lt;/strong&gt; and &lt;strong&gt;update()&lt;/strong&gt; methods anymore so you can delete them. The significant change happens inside the &lt;strong&gt;initializeBuffers()&lt;/strong&gt; method:&lt;/p&gt;

&lt;div class=&quot;language-swift highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kd&quot;&gt;func&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;initializeBuffers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;..&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;particleCount&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;particle&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Particle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;position&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;float2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;Float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arc4random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;  &lt;span class=&quot;kt&quot;&gt;UInt32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;side&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arc4random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;UInt32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;side&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))),&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;velocity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;float2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;Float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arc4random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;Float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arc4random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;particles&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;particle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;particles&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;MemoryLayout&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;Particle&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;particleBuffer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;makeBuffer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;bytes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;particles&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;options&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[])&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;Note: we generate random positions to fill the entire window and we also generate velocities that will range between &lt;code class=&quot;highlighter-rouge&quot;&gt;[-5, 5]&lt;/code&gt;. we also divide by &lt;code class=&quot;highlighter-rouge&quot;&gt;10&lt;/code&gt; to slow them down a little.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The most impoartant part however, is happening when configuring the command encoder. We set the numbers of &lt;code class=&quot;highlighter-rouge&quot;&gt;threads per group&lt;/code&gt; to be a &lt;code class=&quot;highlighter-rouge&quot;&gt;2D&lt;/code&gt; grid determined on one side by the &lt;code class=&quot;highlighter-rouge&quot;&gt;thread execution width&lt;/code&gt; and on the other side by the &lt;code class=&quot;highlighter-rouge&quot;&gt;maximum total threads per threadgroup&lt;/code&gt; which are hardware characteristics specific to each &lt;code class=&quot;highlighter-rouge&quot;&gt;GPU&lt;/code&gt; and will never change during execution. We set the number of &lt;code class=&quot;highlighter-rouge&quot;&gt;threads per grid&lt;/code&gt; to be a one-dimensional array whose size is determined by the particle count:&lt;/p&gt;

&lt;div class=&quot;language-swift highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pipelineState&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;threadExecutionWidth&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pipelineState&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maxTotalThreadsPerThreadgroup&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;threadsPerGroup&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;MTLSizeMake&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;threadsPerGrid&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;MTLSizeMake&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;particleCount&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;commandEncoder&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;dispatchThreads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;threadsPerGrid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;threadsPerThreadgroup&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;threadsPerGroup&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;Note: new in &lt;code class=&quot;highlighter-rouge&quot;&gt;Metal 2&lt;/code&gt;, the &lt;strong&gt;dispatchThreads(:)&lt;/strong&gt; method lets us dispatch work without having to specify how many thread groups we want. In contrast to using the older &lt;strong&gt;dispatchThreadgroups(:)&lt;/strong&gt; method, the new method calculates the number of groups and provides &lt;code class=&quot;highlighter-rouge&quot;&gt;nonuniform thread groups&lt;/code&gt; when the size of the grid is not a multiple of the group size, and also makes sure there are no underutilized threads.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;On to the kernel shader, we first match the particle struct with the one on the &lt;code class=&quot;highlighter-rouge&quot;&gt;CPU&lt;/code&gt; and then inside the kernel we update the positions and velocities:&lt;/p&gt;

&lt;div class=&quot;language-swift highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;Particle&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;particle&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;particles&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;float2&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;position&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;particle&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;position&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;float2&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;velocity&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;particle&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;velocity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;width&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;get_width&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;height&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;get_height&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;position&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;position&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;width&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;velocity&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;position&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;position&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;height&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;velocity&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;position&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;velocity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;particle&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;position&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;position&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;particle&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;velocity&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;velocity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;particles&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;particle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;uint2&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pos&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;uint2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;position&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;position&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;write&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;half4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;write&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;half4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pos&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;uint2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;write&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;half4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pos&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;uint2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;write&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;half4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pos&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;uint2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;write&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;half4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pos&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;uint2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;blockquote&gt;
  &lt;p&gt;Note: we do checks for bounds and when that happens we simply reverse the velocity do the particle does not leave the screen. we also use a neat trick when drawing, by making sure the four neighboring particles are also drawn so they look a bit larger.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;You can set &lt;strong&gt;particleCount&lt;/strong&gt; to &lt;code class=&quot;highlighter-rouge&quot;&gt;1,000,000&lt;/code&gt; if you want but it will take a few seconds to generate them before rendering them all. Because I am only rendering in a relatively small window, I am only rendering &lt;code class=&quot;highlighter-rouge&quot;&gt;10,000&lt;/code&gt; particles so they don’t look too crammed in this window space. If you run the app, you should be able to see the particles falling down like a water stream:&lt;/p&gt;

&lt;p&gt;￼￼&lt;img src=&quot;https://github.com/MetalKit/images/blob/master/particles3.gif?raw=true&quot; alt=&quot;alt text&quot; title=&quot;Particle&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This article concludes the rendering particles series. I want to thank &lt;a href=&quot;https://twitter.com/flexmonkey&quot;&gt;FlexMonkey&lt;/a&gt; for sharing great insights about compute concepts. The &lt;a href=&quot;https://github.com/MetalKit/metal&quot;&gt;source code&lt;/a&gt; is posted on &lt;code class=&quot;highlighter-rouge&quot;&gt;Github&lt;/code&gt; as usual.&lt;/p&gt;

&lt;p&gt;Until next time!&lt;/p&gt;</content><author><name>&lt;a href = &quot;https://twitter.com/gpu3d&quot; target=&quot;_blank&quot;&gt;Marius Horga&lt;/a&gt;</name></author><summary type="html">Last time we looked at how to manipulate vertices from Model I/O objects on the GPU. In this part we are going to show yet another way to create particles using compute threads. We can reuse the playground from last time and we start by modifying the Particle struct in our metal view delegate class to only include two members that we will update on the GPU - position and velocity:</summary></entry><entry><title type="html">Working with Particles in Metal part 2</title><link href="http://localhost:4000/2017/10/31/working-with-particles-in-metal-part-2.html" rel="alternate" type="text/html" title="Working with Particles in Metal part 2" /><published>2017-10-31T00:00:00-05:00</published><updated>2017-10-31T00:00:00-05:00</updated><id>http://localhost:4000/2017/10/31/working-with-particles-in-metal-part-2</id><content type="html" xml:base="http://localhost:4000/2017/10/31/working-with-particles-in-metal-part-2.html">Last time we looked at how to quickly prototype a particle-like object directly inside a shader, using distance functions. That was acceptable for moving an object based on time elapsed. However, if we want to work with vertices we would need to define the particles on the `CPU` and send the vertex data to the `GPU`. We use again a minimal playground we used in the past for `3D` rendering, and we start by creating a __Particle__ struct in our metal view delegate class:

```swift
struct Particle {
    var initialMatrix = matrix_identity_float4x4
    var matrix = matrix_identity_float4x4
    var color = float4()
}
```

Next, we create an array of particles and a buffer to hold the data. Here we also give each particle a nice blue color and a random position to start at:

```swift
particles = [Particle](repeatElement(Particle(), count: 1000))
particlesBuffer = device.makeBuffer(length: particles.count * MemoryLayout&lt;Particle&gt;.stride, options: [])!
var pointer = particlesBuffer.contents().bindMemory(to: Particle.self, capacity: particles.count)
for _ in particles {
    pointer.pointee.initialMatrix = translate(by: [Float(drand48()) / 10, Float(drand48()) * 10, 0])
    pointer.pointee.color = float4(0.2, 0.6, 0.9, 1)
    pointer = pointer.advanced(by: 1)
}
```

&gt; Note: we divide the `x` coordinate by `10` to gather particles inside a small horizontal range, while we multiply the `y` coordinate by `10` for the opposite effect - to spread out the particles vertically a little. 

The next step is to create a sphere that will serve as the particle's mesh:

```swift
let allocator = MTKMeshBufferAllocator(device: device)
let sphere = MDLMesh(sphereWithExtent: [0.01, 0.01, 0.01], segments: [8, 8], inwardNormals: false, geometryType: .triangles, allocator: allocator)
do { model = try MTKMesh(mesh: sphere, device: device) } 
catch let e { print(e) }
```

Next, we need an updating function to animate the particles on the screen. Inside, we increase the timer each frame by `0.01` and update the `y` coordinate using the timer value - creating a falling-like motion:

```swift
func update() {
    timer += 0.01
    var pointer = particlesBuffer.contents().bindMemory(to: Particle.self, capacity: particles.count)
    for _ in particles {
        pointer.pointee.matrix = translate(by: [0, -3 * timer, 0]) * pointer.pointee.initialMatrix
        pointer = pointer.advanced(by: 1)
    }
}
```

At this point we are ready to call this function inside the __draw__ method and then send the data to the `GPU`:

```swift
update()
let submesh = model.submeshes[0]
commandEncoder.setVertexBuffer(model.vertexBuffers[0].buffer, offset: 0, index: 0)
commandEncoder.setVertexBuffer(particlesBuffer, offset: 0, index: 1)
commandEncoder.drawIndexedPrimitives(type: .triangle, indexCount: submesh.indexCount, indexType: submesh.indexType, indexBuffer: submesh.indexBuffer.buffer, indexBufferOffset: 0, instanceCount: particles.count)
```

In the __Shaders.metal__ file we have a struct for the incoming and outgoing vertices, as well as one for the particle instances:

```clike
struct VertexIn {
    float4 position [[attribute(0)]];
};

struct VertexOut {
    float4 position [[position]];
    float4 color;
};

struct Particle {
    float4x4 initial_matrix;
    float4x4 matrix;
    float4 color;
};
```

The vertex shader uses the __instance_id__ attribute which we use to create many instances of the same one sphere we sent to the `GPU` in the vertex buffer at index `0`. We then assign to each instance one of the positions we stored and sent to the `GPU` in the buffer at index `1`.

```clike
vertex VertexOut vertex_main(const VertexIn vertex_in [[stage_in]],
                             constant Particle *particles [[buffer(1)]],
                             uint instanceid [[instance_id]]) {
    VertexOut vertex_out;
    Particle particle = particles[instanceid];
    vertex_out.position = particle.matrix * vertex_in.position ;
    vertex_out.color = particle.color;
    return vertex_out;
}
```

Finally, in the fragment shader we return the color we passed through in the vertex shader:

```clike
fragment float4 fragment_main(VertexOut vertex_in [[stage_in]]) {
    return vertex_in.color;
}
```

If you run the app, you should be able to see the particles falling down like a water stream:

￼￼![alt text](https://github.com/MetalKit/images/blob/master/particles.gif?raw=true &quot;Particle&quot;)

There is yet another, much more efficient approach to rendering particles on the `GPU`. We'll look into that next time. I want to thank [Caroline](https://twitter.com/carolinebegbie) for her valuable assistance with instancing. The [source code](https://github.com/MetalKit/metal) is posted on `Github` as usual.
 
Until next time!</content><author><name>&lt;a href = &quot;https://twitter.com/gpu3d&quot; target=&quot;_blank&quot;&gt;Marius Horga&lt;/a&gt;</name></author><summary type="html">Last time we looked at how to quickly prototype a particle-like object directly inside a shader, using distance functions. That was acceptable for moving an object based on time elapsed. However, if we want to work with vertices we would need to define the particles on the CPU and send the vertex data to the GPU. We use again a minimal playground we used in the past for 3D rendering, and we start by creating a Particle struct in our metal view delegate class:</summary></entry><entry><title type="html">Working with Particles in Metal</title><link href="http://localhost:4000/2017/09/30/working-with-particles-in-metal.html" rel="alternate" type="text/html" title="Working with Particles in Metal" /><published>2017-09-30T00:00:00-05:00</published><updated>2017-09-30T00:00:00-05:00</updated><id>http://localhost:4000/2017/09/30/working-with-particles-in-metal</id><content type="html" xml:base="http://localhost:4000/2017/09/30/working-with-particles-in-metal.html">Today, we're going to start a new series about particles in `Metal`. Since most of the time particles are tiny objects, we are not usually concerned about their geometry. This makes them fit for a compute shader because later on we will want to have granular control over particle-particle interactions and this is a case fit for a high degree of parallelism control which a compute shader allows us to have. Let's use the last playground we worked on when we did ambient occlusion and continue from there. That playground is useful here because it already has a __time__ variable that the `CPU` passes to the `GPU`. Let's start with a fresh __Shaders.metal__ file, and just give the background a nice color:

{% highlight swift %}#include &lt;metal_stdlib&gt;
using namespace metal;

kernel void compute(texture2d&lt;float, access::write&gt; output [[texture(0)]],
                    constant float &amp;time [[buffer(0)]],
                    uint2 gid [[thread_position_in_grid]]) {
    float width = output.get_width();
    float height = output.get_height();
    float2 uv = float2(gid) / float2(width, height);
    float aspect = width / height;
    uv.x *= aspect;
    output.write(float4(0.2, 0.5, 0.7, 1), gid);
}
{% endhighlight %}

Next, let's create a particle object that only has a position (center) and a radius:

{% highlight swift %}struct Particle {
    float2 center;
    float radius;
};
{% endhighlight %}

We also need a way to know where the particle is on the screen, so let's create a distance function for that:

{% highlight swift %}float distanceToParticle(float2 point, Particle p) {
    return length(point - p.center) - p.radius;
}
{% endhighlight %}

Inside the kernel, right above the last line, let's create a new particle and place it at the top of the screen, midway on the `X` axis. Give it a radius of `0.05`:

{% highlight swift %}float2 center = float2(aspect / 2, time);
float radius = 0.05;
Particle p = Particle{center, radius};
{% endhighlight %}

&gt; Note: we used the `time` as the `Y` coordinate of the particle but this is only a trick to show basic movement. Soon, we will replace this variable with a coordinate that changes under the laws of physics. 

Replace the last line of the kernel with these lines and run the app. You should see the particle falling down at a steady rate:

{% highlight swift %}float distance = distanceToParticle(uv, p);
float4 color = float4(1, 0.7, 0, 1);
if (distance &gt; 0) { color = float4(0.2, 0.5, 0.7, 1); }
output.write(float4(color), gid);
{% endhighlight %}

The particle, however, will keep going down forever. To make it stop at the bottom, enforce this condition right before creating the particle:

{% highlight swift %}float stop = 1 - radius;
if (time &gt;= stop) { center.y = stop; }
else center.y = time;
{% endhighlight %}

&gt; Note: both `time` and the `uv` variables go from `0-1` so we create a `stop` point which is the window height less the particle radius. 

That was a very basic collision detection rule. If you run the app, you should be able to see the particle falling down uniformly and stopping at the bottom, like this:

￼￼![alt text](https://github.com/MetalKit/images/blob/master/particle.gif?raw=true &quot;Particle&quot;)

Next time we will go deeper into particle dynamics and implement the laws of motion from physics. The [source code](https://github.com/MetalKit/metal) is posted on `Github` as usual.
 
Until next time!</content><author><name>&lt;a href = &quot;https://twitter.com/gpu3d&quot; target=&quot;_blank&quot;&gt;Marius Horga&lt;/a&gt;</name></author><summary type="html">Today, we’re going to start a new series about particles in Metal. Since most of the time particles are tiny objects, we are not usually concerned about their geometry. This makes them fit for a compute shader because later on we will want to have granular control over particle-particle interactions and this is a case fit for a high degree of parallelism control which a compute shader allows us to have. Let’s use the last playground we worked on when we did ambient occlusion and continue from there. That playground is useful here because it already has a time variable that the CPU passes to the GPU. Let’s start with a fresh Shaders.metal file, and just give the background a nice color:</summary></entry><entry><title type="html">Using ARKit with Metal part 2</title><link href="http://localhost:4000/2017/08/31/using-arkit-with-metal-part-2.html" rel="alternate" type="text/html" title="Using ARKit with Metal part 2" /><published>2017-08-31T00:00:00-05:00</published><updated>2017-08-31T00:00:00-05:00</updated><id>http://localhost:4000/2017/08/31/using-arkit-with-metal-part-2</id><content type="html" xml:base="http://localhost:4000/2017/08/31/using-arkit-with-metal-part-2.html">As underlined last time, ￼there are three layers in an __ARKit__ application: `Rendering`, `Tracking` and `Scene Understanding`. Last time we analyzed in great detail how _Rendering_ is done in `Metal` using a custom view. `ARKit` uses `Visual Inertial Odometry` for accurate _Tracking_ of the world around it and to combine camera sensor data with `CoreMotion` data. No additional calibration is necessary for image stability while we are in motion. In this article we look at **Scene Understanding** - ways of describing scene attributes by using plane detection, hit-testing and light estimation. `ARKit` can analyze the scene presented by the camera view and find horizontal planes such as floors. First, we need to enable the plane detection feature (which is __off__ by default) by simply adding one more line before running the session configuration:

{% highlight swift %}override func viewWillAppear(_ animated: Bool) {
    super.viewWillAppear(animated)
    let configuration = ARWorldTrackingConfiguration()
    configuration.planeDetection = .horizontal
    session.run(configuration)
}
{% endhighlight %}

&gt; Note that only __horizontal__ plane detection is possible with the current `API` version.

The __ARSessionObserver__ protocol's methods are used for handling session errors, tracking changes and interruptions:
    
{% highlight swift %}func session(_ session: ARSession, didFailWithError error: Error) {}
func session(_ session: ARSession, cameraDidChangeTrackingState camera: ARCamera) {}
func session(_ session: ARSession, didOutputAudioSampleBuffer audioSampleBuffer: CMSampleBuffer) {}
func sessionWasInterrupted(_ session: ARSession) {}
func sessionInterruptionEnded(_ session: ARSession) {}
{% endhighlight %}

However, there are other delegate methods that belong to the __ARSessionDelegate__ protocol (which extends `ARSessionObserver`) that let us work with anchors. Put a __print()__ call inside the first one:

{% highlight swift %}func session(_ session: ARSession, didAdd anchors: [ARAnchor]) {
    print(anchors)
}
func session(_ session: ARSession, didRemove anchors: [ARAnchor]) {}
func session(_ session: ARSession, didUpdate anchors: [ARAnchor]) {}
func session(_ session: ARSession, didUpdate frame: ARFrame) {}
{% endhighlight %}

Let's move to the __Renderer.swift__ file now. First, create a few class properties we need to work with. These variables will help us create and display a debug plane on the screen:

{% highlight swift %}var debugUniformBuffer: MTLBuffer!
var debugPipelineState: MTLRenderPipelineState!
var debugDepthState: MTLDepthStencilState!var debugMesh: MTKMesh!
var debugUniformBufferOffset: Int = 0
var debugUniformBufferAddress: UnsafeMutableRawPointer!
var debugInstanceCount: Int = 0
{% endhighlight %}

Next, in __setupPipeline()__ we create the buffer:

{% highlight swift %}debugUniformBuffer = device.makeBuffer(length: anchorUniformBufferSize, options: .storageModeShared)
{% endhighlight %}

We need to create new vertex and fragment functions for our plane, as well as new render pipeline and depth stencil states. Right before the line where the command queue is created, add these lines:

{% highlight swift %}let debugGeometryVertexFunction = defaultLibrary.makeFunction(name: &quot;vertexDebugPlane&quot;)!
let debugGeometryFragmentFunction = defaultLibrary.makeFunction(name: &quot;fragmentDebugPlane&quot;)!
anchorPipelineStateDescriptor.vertexFunction =  debugGeometryVertexFunction
anchorPipelineStateDescriptor.fragmentFunction = debugGeometryFragmentFunction
do { try debugPipelineState = device.makeRenderPipelineState(descriptor: anchorPipelineStateDescriptor)
} catch let error { print(error) }
debugDepthState = device.makeDepthStencilState(descriptor: anchorDepthStateDescriptor)
{% endhighlight %}

Next, in __setupAssets()__ we need to create a new `Model I/O` plane mesh and then create the `Metal` mesh from it. At the end of the function add these lines:

{% highlight swift %}mdlMesh = MDLMesh(planeWithExtent: vector3(0.1, 0.1, 0.1), segments: vector2(1, 1), geometryType: .triangles, allocator: metalAllocator)
mdlMesh.vertexDescriptor = vertexDescriptor
do { try debugMesh = MTKMesh(mesh: mdlMesh, device: device)
} catch let error { print(error) }
{% endhighlight %}

Next, in __updateBufferStates()__ we need to update the address of the buffer where the plane resides. Add the following lines:

{% highlight swift %}debugUniformBufferOffset = alignedInstanceUniformSize * uniformBufferIndex
debugUniformBufferAddress = debugUniformBuffer.contents().advanced(by: debugUniformBufferOffset)
{% endhighlight %}

Next, in __updateAnchors()__ we need to update the transform matrices and the anchors count. Add the following lines before the loop:

{% highlight swift %}let count = frame.anchors.filter{ $0.isKind(of: ARPlaneAnchor.self) }.count
debugInstanceCount = min(count, maxAnchorInstanceCount - (anchorInstanceCount - count))
{% endhighlight %}

Then, inside the loop replace the last three lines with the following lines:

{% highlight swift %}if anchor.isKind(of: ARPlaneAnchor.self) {
    let transform = anchor.transform * rotationMatrix(rotation: float3(0, 0, Float.pi/2))
    let modelMatrix = simd_mul(transform, coordinateSpaceTransform)
    let debugUniforms = debugUniformBufferAddress.assumingMemoryBound(to: InstanceUniforms.self).advanced(by: index)
    debugUniforms.pointee.modelMatrix = modelMatrix
} else {
    let modelMatrix = simd_mul(anchor.transform, coordinateSpaceTransform)
    let anchorUniforms = anchorUniformBufferAddress.assumingMemoryBound(to: InstanceUniforms.self).advanced(by: index)
    anchorUniforms.pointee.modelMatrix = modelMatrix
}
{% endhighlight %}

We had to rotate the plane __90__ degrees by the __Z__ axis so we can make it `horizontal`. Notice that we used a custom method named __rotationMatrix()__ so let's define it. We have seen this matrix in the early articles when we first introduced `3D` transforms:

{% highlight swift %}func rotationMatrix(rotation: float3) -&gt; float4x4 {
    var matrix: float4x4 = matrix_identity_float4x4
    let x = rotation.x
    let y = rotation.y
    let z = rotation.z
    matrix.columns.0.x = cos(y) * cos(z)
    matrix.columns.0.y = cos(z) * sin(x) * sin(y) - cos(x) * sin(z)
    matrix.columns.0.z = cos(x) * cos(z) * sin(y) + sin(x) * sin(z)
    matrix.columns.1.x = cos(y) * sin(z)
    matrix.columns.1.y = cos(x) * cos(z) + sin(x) * sin(y) * sin(z)
    matrix.columns.1.z = -cos(z) * sin(x) + cos(x) * sin(y) * sin(z)
    matrix.columns.2.x = -sin(y)
    matrix.columns.2.y = cos(y) * sin(x)
    matrix.columns.2.z = cos(x) * cos(y)
    matrix.columns.3.w = 1.0
    return matrix
}
{% endhighlight %}

Next, in __drawAnchorGeometry()__ we need to make sure we have at least one anchor before drawing it. Replace the first line with this one:

{% highlight swift %}guard anchorInstanceCount - debugInstanceCount &gt; 0 else { return }
{% endhighlight %}

Next, let's finally create the __drawDebugGeometry()__ function that draws our plane. It's very similar to the anchor drawing function:

{% highlight swift %}func drawDebugGeometry(renderEncoder: MTLRenderCommandEncoder) {
    guard debugInstanceCount &gt; 0 else { return }
    renderEncoder.pushDebugGroup(&quot;DrawDebugPlanes&quot;)
    renderEncoder.setCullMode(.back)
    renderEncoder.setRenderPipelineState(debugPipelineState)
    renderEncoder.setDepthStencilState(debugDepthState)
    renderEncoder.setVertexBuffer(debugUniformBuffer, offset: debugUniformBufferOffset, index: 2)
    renderEncoder.setVertexBuffer(sharedUniformBuffer, offset: sharedUniformBufferOffset, index: 3)
    renderEncoder.setFragmentBuffer(sharedUniformBuffer, offset: sharedUniformBufferOffset, index: 3)
    for bufferIndex in 0..&lt;debugMesh.vertexBuffers.count {
        let vertexBuffer = debugMesh.vertexBuffers[bufferIndex]
        renderEncoder.setVertexBuffer(vertexBuffer.buffer, offset: vertexBuffer.offset, index:bufferIndex)
    }
    for submesh in debugMesh.submeshes {
        renderEncoder.drawIndexedPrimitives(type: submesh.primitiveType, indexCount: submesh.indexCount, indexType: submesh.indexType, indexBuffer: submesh.indexBuffer.buffer, indexBufferOffset: submesh.indexBuffer.offset, instanceCount: debugInstanceCount)
    }
    renderEncoder.popDebugGroup()
}
{% endhighlight %}

There is one more thing left to do in `Renderer` and that is - call this function in __update()__ right above the line where we end the encoding:

{% highlight swift %}drawDebugGeometry(renderEncoder: renderEncoder)
{% endhighlight %}

Finally, let's go to the __Shaders.metal__ file. We need a new struct with just the vertex position passed via a vertex descriptor:

{% highlight swift %}typedef struct {
    float3 position [[attribute(0)]];
} DebugVertex;
{% endhighlight %}

In the vertex shader we update the vertex position using the model-view matrix:

{% highlight swift %}vertex float4 vertexDebugPlane(DebugVertex in [[ stage_in]],
                               constant SharedUniforms &amp;sharedUniforms [[ buffer(3) ]],
                               constant InstanceUniforms *instanceUniforms [[ buffer(2) ]],
                               ushort vid [[vertex_id]],
                               ushort iid [[instance_id]]) {
    float4 position = float4(in.position, 1.0);
    float4x4 modelMatrix = instanceUniforms[iid].modelMatrix;
    float4x4 modelViewMatrix = sharedUniforms.viewMatrix * modelMatrix;
    float4 outPosition = sharedUniforms.projectionMatrix * modelViewMatrix * position;
    return outPosition;
}
{% endhighlight %}

And last, in the fragment shader we give the plane a bold color to make it noticeable in the view:

{% highlight swift %}fragment float4 fragmentDebugPlane() {
    return float4(0.99, 0.42, 0.62, 1.0);
}
{% endhighlight %}

If you run the app, you should be able to see a rectangle added when the app detects a plane, like this:

￼￼![alt text](https://github.com/MetalKit/images/blob/master/plane.gif?raw=true &quot;Plane detection&quot;)

What we could do next is update/remove planes as we detect more or as we move away from the previously detected one. The other delegate methods can help us achieve just that. Then, we could look at collisions and physics. Just a thought for the future. 

I want to thank [Caroline](https://twitter.com/carolinebegbie) for being the designated (plane) detective for this article! The [source code](https://github.com/MetalKit/metal) is posted on `Github` as usual.
 
Until next time!</content><author><name>&lt;a href = &quot;https://twitter.com/gpu3d&quot; target=&quot;_blank&quot;&gt;Marius Horga&lt;/a&gt;</name></author><summary type="html">As underlined last time, ￼there are three layers in an ARKit application: Rendering, Tracking and Scene Understanding. Last time we analyzed in great detail how Rendering is done in Metal using a custom view. ARKit uses Visual Inertial Odometry for accurate Tracking of the world around it and to combine camera sensor data with CoreMotion data. No additional calibration is necessary for image stability while we are in motion. In this article we look at Scene Understanding - ways of describing scene attributes by using plane detection, hit-testing and light estimation. ARKit can analyze the scene presented by the camera view and find horizontal planes such as floors. First, we need to enable the plane detection feature (which is off by default) by simply adding one more line before running the session configuration:</summary></entry><entry><title type="html">Using ARKit with Metal</title><link href="http://localhost:4000/2017/07/29/using-arkit-with-metal.html" rel="alternate" type="text/html" title="Using ARKit with Metal" /><published>2017-07-29T00:00:00-05:00</published><updated>2017-07-29T00:00:00-05:00</updated><id>http://localhost:4000/2017/07/29/using-arkit-with-metal</id><content type="html" xml:base="http://localhost:4000/2017/07/29/using-arkit-with-metal.html">**Augmented Reality** provides a way of overlaying virtual content on top of real world views usually obtained from a mobile device camera. Last month at `WWDC 2017` we were all thrilled to see `Apple`'s new **ARKit** framework which is a high level `API` that works with `A9`-powered devices or newer, running on `iOS 11`. Some of the ARKit experiments we've already seen are outstanding, such as this one below: 

￼￼![alt text](https://github.com/MetalKit/images/blob/master/ARKit.gif?raw=true &quot;ARKit&quot;)

There are three distinct layers in an `ARKit` application:

- **Tracking** - no external setup is necessary to do world tracking using visual inertial odometry.
- **Scene Understanding** - the ability of detecting scene attributes using plane detection, hit-testing and light estimation.
- **Rendering** - can be easily integrated because of the template `AR` views provided by `SpriteKit` and `SceneKit` but it can also be customized for `Metal`. All the pre-render processing is done by `ARKit` which is also responsible for image capturing using `AVFoundation` and `CoreMotion`.

In this first part of the series we will be looking mostly at `Rendering` in `Metal` and talk about the other two stages in the next part of this series. In an `AR` application, the `Tracking` and `Scene Understanding` are handled entirely by the `ARKit` framework while `Rendering` can be handled by either `SpriteKit`, `SceneKit` or `Metal`: 

￼￼![alt text](https://github.com/MetalKit/images/blob/master/ARKit1.png?raw=true &quot;ARKit 1&quot;)

To get started, we need to have an **ARSession** instance that is set up by an **ARSessionConfiguration** object. Then, we call the __run()__ function on this configuration. The session also has __AVCaptureSession__ and __CMMotionManager__ objects running at the same time to get image and motion data for tracking. Finally, the session will output the current frame to an __ARFrame__ object:

![alt text](https://github.com/MetalKit/images/blob/master/ARKit2.png?raw=true &quot;ARKit 2&quot;)

The `ARSessionConfiguration` object contains information about the type of tracking the session will have. The `ARSessionConfiguration` base configuration class provides __3__ degrees of freedom tracking (the device _orientation_) while its subclass, __ARWorldTrackingSessionConfiguration__, provides __6__ degrees of freedom tracking (the device _position_ and _orientation_). 

![alt text](https://github.com/MetalKit/images/blob/master/ARKit4.png?raw=true &quot;ARKit 4&quot;)

When a device does not support world tracking, it falls back to the base  configuration:

{% highlight swift %}if ARWorldTrackingSessionConfiguration.isSupported { 
    configuration = ARWorldTrackingSessionConfiguration()
} else {
    configuration = ARSessionConfiguration() 
}
{% endhighlight %}

An `ARFrame` contains the captured image, tracking information and well as scene information via __ARAnchor__ objects that contain information about real world position and orientation and can be easily added, updated or removed from sessions. `Tracking` is the ability to determine the physical location in real time. The `World Tracking` however, determines both position and orientation, it works with physical distances, it’s relative to the starting position and provides `3D`-feature points. 

The last component of an `ARFrame` are __ARCamera__ objects which facilitate transforms (translation, rotation, scaling) and carry tracking state and camera intrinsics. The quality of tracking relies heavily on uninterrupted sensor data, static scenes and is more accurate when scenes have textured environment with plenty of complexity. Tracking state has three values: __Not Available__ (camera only has the identity matrix), __Limited__ (scene has insufficient features or is not static enough) and __Normal__ (camera is populated with data). Session interruptions are caused by camera input not being available or when tracking is stopped:

{% highlight swift %}func session(_ session: ARSession, cameraDidChangeTrackingState camera: ARCamera) { 
    if case .limited(let reason) = camera.trackingState {
        // Notify user of limited tracking state
    } 
}
func sessionWasInterrupted(_ session: ARSession) { 
    showOverlay()
}
func sessionInterruptionEnded(_ session: ARSession) { 
    hideOverlay()
    // Optionally restart experience
}
{% endhighlight %}

`Rendering` can be done in `SceneKit` using the `ARSCNView`'s delegate to add, update or remove nodes. Similarly, rendering can be done in `SpriteKit` using the `ARSKView` delegate which maps `SKNodes` to `ARAnchor` objects. Since `SpriteKit` is `2D`, it cannot use the real world camera position, so it projects the anchor positions into the `ARSKView` and then renders the sprite as a billboard (plane) at this projected location, so the sprite will always be facing the camera. For `Metal`, there is no customized `AR` view so that responsibility falls in programmer’s hands. For processing of rendered images we need to: 

- draw background camera image (generate a texture from the pixel buffer)
- update the virtual camera
- update the lighting
- update the transforms for geometry

All this information is in the `ARFrame` object. To access the frame, there are two options: _polling_ or using a _delegate_. We are going to describe the latter. I took the `ARKit` template for `Metal` and stripped it down to a minimum so I can better understand how it works. First thing I did was to remove all the `C` dependencies so bridging is not necessary anymore. It will be useful in the future to have it in place so types and enum constants can be shared between `API` code and shaders but for the purpose of this article it is not needed.

Next, on to __ViewController__ which will act as both our `MTKView` and `ARSession` delegates. We create a `Renderer` instance that will work with the delegates for real time updates to the application: 

{% highlight swift %}var session: ARSession!
var renderer: Renderer!

override func viewDidLoad() {
    super.viewDidLoad()
    session = ARSession()
    session.delegate = self
    if let view = self.view as? MTKView {
        view.device = MTLCreateSystemDefaultDevice()
        view.delegate = self
        renderer = Renderer(session: session, metalDevice: view.device!, renderDestination: view)
        renderer.drawRectResized(size: view.bounds.size)
    }
    let tapGesture = UITapGestureRecognizer(target: self, action: #selector(self.handleTap(gestureRecognize:)))
    view.addGestureRecognizer(tapGesture)
}
{% endhighlight %}

As you can see, we also added a gesture recognizer which we will use to add virtual content to our view. We first get the session’s current frame, then create a translation to put our object in front of the camera (__0.3__ meters in this case) and finally add a new anchor to our session using this transform:
    
{% highlight swift %}func handleTap(gestureRecognize: UITapGestureRecognizer) {
    if let currentFrame = session.currentFrame {
        var translation = matrix_identity_float4x4
        translation.columns.3.z = -0.3
        let transform = simd_mul(currentFrame.camera.transform, translation)
        let anchor = ARAnchor(transform: transform)
        session.add(anchor: anchor)
    }
}
{% endhighlight %}

We use the __viewWillAppear()__ and __viewWillDisappear()__ methods to start and pause the session:
    
{% highlight swift %}override func viewWillAppear(_ animated: Bool) {
    super.viewWillAppear(animated)
    let configuration = ARWorldTrackingSessionConfiguration()
    session.run(configuration)
}

override func viewWillDisappear(_ animated: Bool) {
    super.viewWillDisappear(animated)
    session.pause()
}
{% endhighlight %}

What’s left is only the delegate methods which we need to react to view updates or session errors and interruptions:
    
{% highlight swift %}func mtkView(_ view: MTKView, drawableSizeWillChange size: CGSize) {
    renderer.drawRectResized(size: size)
}

func draw(in view: MTKView) {
    renderer.update()
}

func session(_ session: ARSession, didFailWithError error: Error) {}

func sessionWasInterrupted(_ session: ARSession) {}

func sessionInterruptionEnded(_ session: ARSession) {}
{% endhighlight %}

Let's move to the __Renderer.swift__ file now. The first thing to notice is the use of a very handy protocol that will give us access to all the `MTKView` properties we need for the draw call later:

{% highlight swift %}protocol RenderDestinationProvider {
    var currentRenderPassDescriptor: MTLRenderPassDescriptor? { get }
    var currentDrawable: CAMetalDrawable? { get }
    var colorPixelFormat: MTLPixelFormat { get set }
    var depthStencilPixelFormat: MTLPixelFormat { get set }
    var sampleCount: Int { get set }
}
{% endhighlight %}

Now you can simply extend the `MTKView` class (in `ViewController`) so it conforms to this protocol:

{% highlight swift %}extension MTKView : RenderDestinationProvider {}
{% endhighlight %}

To have a high level view of the `Renderer` class, here is the pseudocode:

{% highlight swift %}init() {
    setupPipeline()
    setupAssets()
}
    
func update() {
    updateBufferStates()
    updateSharedUniforms()
    updateAnchors()
    updateCapturedImageTextures()
    updateImagePlane()
    drawCapturedImage()
    drawAnchorGeometry()
}
{% endhighlight %}
    
As always, we first setup the pipeline, here with the __setupPipeline()__ function. Then, in __setupAssets()__ we create our model which will be loaded every time we use our tap gesture recognizer. The `MTKView` delegate will call the __update()__ function for the needed updates and draw calls. Let's look at each of them in detail. First we have __updateBufferStates()__ which updates the locations we write to in our buffers for the current frame (we use a ring buffer with __3__ slots in this case):
       
{% highlight swift %}func updateBufferStates() {
    uniformBufferIndex = (uniformBufferIndex + 1) % maxBuffersInFlight
    sharedUniformBufferOffset = alignedSharedUniformSize * uniformBufferIndex
    anchorUniformBufferOffset = alignedInstanceUniformSize * uniformBufferIndex
    sharedUniformBufferAddress = sharedUniformBuffer.contents().advanced(by: sharedUniformBufferOffset)
    anchorUniformBufferAddress = anchorUniformBuffer.contents().advanced(by: anchorUniformBufferOffset)
}
{% endhighlight %}

Next, in __updateSharedUniforms()__ we update the shared uniforms of the frame and set up lighting for the scene:

{% highlight swift %}func updateSharedUniforms(frame: ARFrame) {
    let uniforms = sharedUniformBufferAddress.assumingMemoryBound(to: SharedUniforms.self)
    uniforms.pointee.viewMatrix = simd_inverse(frame.camera.transform)
    uniforms.pointee.projectionMatrix = frame.camera.projectionMatrix(withViewportSize: viewportSize, orientation: .landscapeRight, zNear: 0.001, zFar: 1000)
    var ambientIntensity: Float = 1.0
    if let lightEstimate = frame.lightEstimate {
        ambientIntensity = Float(lightEstimate.ambientIntensity) / 1000.0
    }
    let ambientLightColor: vector_float3 = vector3(0.5, 0.5, 0.5)
    uniforms.pointee.ambientLightColor = ambientLightColor * ambientIntensity
    var directionalLightDirection : vector_float3 = vector3(0.0, 0.0, -1.0)
    directionalLightDirection = simd_normalize(directionalLightDirection)
    uniforms.pointee.directionalLightDirection = directionalLightDirection
    let directionalLightColor: vector_float3 = vector3(0.6, 0.6, 0.6)
    uniforms.pointee.directionalLightColor = directionalLightColor * ambientIntensity
    uniforms.pointee.materialShininess = 30
}
{% endhighlight %}

Next, in __updateAnchors()__ we update the anchor uniform buffer with transforms of the current frame's anchors:

{% highlight swift %}func updateAnchors(frame: ARFrame) {
    anchorInstanceCount = min(frame.anchors.count, maxAnchorInstanceCount)
    var anchorOffset: Int = 0
    if anchorInstanceCount == maxAnchorInstanceCount {
        anchorOffset = max(frame.anchors.count - maxAnchorInstanceCount, 0)
    }
    for index in 0..&lt;anchorInstanceCount {
        let anchor = frame.anchors[index + anchorOffset]
        var coordinateSpaceTransform = matrix_identity_float4x4
        coordinateSpaceTransform.columns.2.z = -1.0
        let modelMatrix = simd_mul(anchor.transform, coordinateSpaceTransform)
        let anchorUniforms = anchorUniformBufferAddress.assumingMemoryBound(to: InstanceUniforms.self).advanced(by: index)
        anchorUniforms.pointee.modelMatrix = modelMatrix
    }
}
{% endhighlight %}

Next, in __updateCapturedImageTextures()__ we create two textures from the provided frame's captured image:

{% highlight swift %}func updateCapturedImageTextures(frame: ARFrame) {
    let pixelBuffer = frame.capturedImage
    if (CVPixelBufferGetPlaneCount(pixelBuffer) &lt; 2) { return }
    capturedImageTextureY = createTexture(fromPixelBuffer: pixelBuffer, pixelFormat:.r8Unorm, planeIndex:0)!
    capturedImageTextureCbCr = createTexture(fromPixelBuffer: pixelBuffer, pixelFormat:.rg8Unorm, planeIndex:1)!
}
{% endhighlight %}

Next, in __updateImagePlane()__ we update the texture coordinates of our image plane to aspect fill the viewport:
        
{% highlight swift %}func updateImagePlane(frame: ARFrame) {
    let displayToCameraTransform = frame.displayTransform(withViewportSize: viewportSize, orientation: .landscapeRight).inverted()
    let vertexData = imagePlaneVertexBuffer.contents().assumingMemoryBound(to: Float.self)
    for index in 0...3 {
        let textureCoordIndex = 4 * index + 2
        let textureCoord = CGPoint(x: CGFloat(planeVertexData[textureCoordIndex]), y: CGFloat(planeVertexData[textureCoordIndex + 1]))
        let transformedCoord = textureCoord.applying(displayToCameraTransform)
        vertexData[textureCoordIndex] = Float(transformedCoord.x)
        vertexData[textureCoordIndex + 1] = Float(transformedCoord.y)
    }
}
{% endhighlight %}

Next, in __drawCapturedImage()__ we draw the camera feed in the scene:

{% highlight swift %}func drawCapturedImage(renderEncoder: MTLRenderCommandEncoder) {
    guard capturedImageTextureY != nil &amp;&amp; capturedImageTextureCbCr != nil else { return }
    renderEncoder.pushDebugGroup(&quot;DrawCapturedImage&quot;)
    renderEncoder.setCullMode(.none)
    renderEncoder.setRenderPipelineState(capturedImagePipelineState)
    renderEncoder.setDepthStencilState(capturedImageDepthState)
    renderEncoder.setVertexBuffer(imagePlaneVertexBuffer, offset: 0, index: 0)
    renderEncoder.setFragmentTexture(capturedImageTextureY, index: 1)
    renderEncoder.setFragmentTexture(capturedImageTextureCbCr, index: 2)
    renderEncoder.drawPrimitives(type: .triangleStrip, vertexStart: 0, vertexCount: 4)
    renderEncoder.popDebugGroup()
}
{% endhighlight %}
    
Finally, in __drawAnchorGeometry()__ we draw the anchors for the virtual content we created:

{% highlight swift %}func drawAnchorGeometry(renderEncoder: MTLRenderCommandEncoder) {
    guard anchorInstanceCount &gt; 0 else { return }
    renderEncoder.pushDebugGroup(&quot;DrawAnchors&quot;)
    renderEncoder.setCullMode(.back)
    renderEncoder.setRenderPipelineState(anchorPipelineState)
    renderEncoder.setDepthStencilState(anchorDepthState)
    renderEncoder.setVertexBuffer(anchorUniformBuffer, offset: anchorUniformBufferOffset, index: 2)
    renderEncoder.setVertexBuffer(sharedUniformBuffer, offset: sharedUniformBufferOffset, index: 3)
    renderEncoder.setFragmentBuffer(sharedUniformBuffer, offset: sharedUniformBufferOffset, index: 3)
    for bufferIndex in 0..&lt;mesh.vertexBuffers.count {
        let vertexBuffer = mesh.vertexBuffers[bufferIndex]
        renderEncoder.setVertexBuffer(vertexBuffer.buffer, offset: vertexBuffer.offset, index:bufferIndex)
    }
    for submesh in mesh.submeshes {
        renderEncoder.drawIndexedPrimitives(type: submesh.primitiveType, indexCount: submesh.indexCount, indexType: submesh.indexType, indexBuffer: submesh.indexBuffer.buffer, indexBufferOffset: submesh.indexBuffer.offset, instanceCount: anchorInstanceCount)
    }
    renderEncoder.popDebugGroup()
}
{% endhighlight %}

Back to the __setupPipeline()__ function which we briefly mentioned earlier. We create two render pipeline state objects, one for the captured image (the camera feed) and one for the anchors we create when placing virtual objects in the scene. As expected, each of the state objects will have their own pair of vertex and fragment functions - which brings us to the last file we need to look at - the __Shaders.metal__ file. In the first pair of shaders for the captured image, we pass through the image vertex's position and texture coordinate in the vertex shader:

{% highlight swift %}vertex ImageColorInOut capturedImageVertexTransform(ImageVertex in [[stage_in]]) {
    ImageColorInOut out;
    out.position = float4(in.position, 0.0, 1.0);
    out.texCoord = in.texCoord;
    return out;
}
{% endhighlight %}

In the fragment shader we sample the two textures to get the color at the given texture coordinate after which we return the converted `RGB` color:

{% highlight swift %}fragment float4 capturedImageFragmentShader(ImageColorInOut in [[stage_in]],
                                            texture2d&lt;float, access::sample&gt; textureY [[ texture(1) ]],
                                            texture2d&lt;float, access::sample&gt; textureCbCr [[ texture(2) ]]) {
    constexpr sampler colorSampler(mip_filter::linear, mag_filter::linear, min_filter::linear);
    const float4x4 ycbcrToRGBTransform = float4x4(float4(+1.0000f, +1.0000f, +1.0000f, +0.0000f),
                                                  float4(+0.0000f, -0.3441f, +1.7720f, +0.0000f),
                                                  float4(+1.4020f, -0.7141f, +0.0000f, +0.0000f),
                                                  float4(-0.7010f, +0.5291f, -0.8860f, +1.0000f));
    float4 ycbcr = float4(textureY.sample(colorSampler, in.texCoord).r, textureCbCr.sample(colorSampler, in.texCoord).rg, 1.0);
    return ycbcrToRGBTransform * ycbcr;
}
{% endhighlight %}

In the second pair of shaders for the anchor geometry, in the vertex shader we calculate the position of our vertex in clip space and output for clipping and rasterization, then color each face a different color, then calculate the positon of our vertex in eye space and finally rotate our normals to world coordinates:

{% highlight swift %}vertex ColorInOut anchorGeometryVertexTransform(Vertex in [[stage_in]],
                                                constant SharedUniforms &amp;sharedUniforms [[ buffer(3) ]],
                                                constant InstanceUniforms *instanceUniforms [[ buffer(2) ]],
                                                ushort vid [[vertex_id]],
                                                ushort iid [[instance_id]]) {
    ColorInOut out;
    float4 position = float4(in.position, 1.0);
    float4x4 modelMatrix = instanceUniforms[iid].modelMatrix;
    float4x4 modelViewMatrix = sharedUniforms.viewMatrix * modelMatrix;
    out.position = sharedUniforms.projectionMatrix * modelViewMatrix * position;
    ushort colorID = vid / 4 % 6;
    out.color = colorID == 0 ? float4(0.0, 1.0, 0.0, 1.0)  // Right face
              : colorID == 1 ? float4(1.0, 0.0, 0.0, 1.0)  // Left face
              : colorID == 2 ? float4(0.0, 0.0, 1.0, 1.0)  // Top face
              : colorID == 3 ? float4(1.0, 0.5, 0.0, 1.0)  // Bottom face
              : colorID == 4 ? float4(1.0, 1.0, 0.0, 1.0)  // Back face
              :                float4(1.0, 1.0, 1.0, 1.0); // Front face
    out.eyePosition = half3((modelViewMatrix * position).xyz);
    float4 normal = modelMatrix * float4(in.normal.x, in.normal.y, in.normal.z, 0.0f);
    out.normal = normalize(half3(normal.xyz));
    return out;
}
{% endhighlight %}

In the fragment shader, we calculate the contribution of the directional light as a sum of diffuse and specular terms, then we compute the final color by multiplying the sample from the color maps by the fragment's lighting value and finally use the color we just computed and the alpha channel of the color map for this fragment's alpha value:

{% highlight swift %}fragment float4 anchorGeometryFragmentLighting(ColorInOut in [[stage_in]],
                                               constant SharedUniforms &amp;uniforms [[ buffer(3) ]]) {
    float3 normal = float3(in.normal);
    float3 directionalContribution = float3(0);
    {
        float nDotL = saturate(dot(normal, -uniforms.directionalLightDirection));
        float3 diffuseTerm = uniforms.directionalLightColor * nDotL;
        float3 halfwayVector = normalize(-uniforms.directionalLightDirection - float3(in.eyePosition));
        float reflectionAngle = saturate(dot(normal, halfwayVector));
        float specularIntensity = saturate(powr(reflectionAngle, uniforms.materialShininess));
        float3 specularTerm = uniforms.directionalLightColor * specularIntensity;
        directionalContribution = diffuseTerm + specularTerm;
    }
    float3 ambientContribution = uniforms.ambientLightColor;
    float3 lightContributions = ambientContribution + directionalContribution;
    float3 color = in.color.rgb * lightContributions;
    return float4(color, in.color.w);
}
{% endhighlight %}

If you run the app, you should be able to tap on the screen to add cubes on top of your live camera view, and move away or closer or around the cubes to see their different colors on each face, like this:

￼￼![alt text](https://github.com/MetalKit/images/blob/master/ARKit1.gif?raw=true &quot;ARKit 1&quot;)

In the next part of the series we will look more into `Tracking` and `Scene Understanding` and see how plane detection, hit-testing, collisions and physics can make our experience even greater. The [source code](https://github.com/MetalKit/metal) is posted on `Github` as usual.
 
Until next time!</content><author><name>&lt;a href = &quot;https://twitter.com/gpu3d&quot; target=&quot;_blank&quot;&gt;Marius Horga&lt;/a&gt;</name></author><summary type="html">Augmented Reality provides a way of overlaying virtual content on top of real world views usually obtained from a mobile device camera. Last month at WWDC 2017 we were all thrilled to see Apple’s new ARKit framework which is a high level API that works with A9-powered devices or newer, running on iOS 11. Some of the ARKit experiments we’ve already seen are outstanding, such as this one below:</summary></entry><entry><title type="html">Introducing Metal 2</title><link href="http://localhost:4000/2017/06/30/introducing-metal-2.html" rel="alternate" type="text/html" title="Introducing Metal 2" /><published>2017-06-30T00:00:00-05:00</published><updated>2017-06-30T00:00:00-05:00</updated><id>http://localhost:4000/2017/06/30/introducing-metal-2</id><content type="html" xml:base="http://localhost:4000/2017/06/30/introducing-metal-2.html">This year's `WWDC` was probably the most important one ever, at least as far as we - the `Metal` developers - are concerned. I can wholeheartedly say it was the [best week of my life](https://twitter.com/gpu3d/status/873049387269738497), for sure!

Let's get to the _Games and Graphics_ news. The `most unexpected` trophy goes to the renaming of `Metal` to __Metal 2__. It has the most significant additions and enhancements since it was first announced in `2014`, true, but let's admit it: no one saw this one coming. The `most anticipated` trophy goes to the new __ARKit__ framework. We are only a few weeks after the keynote and there are already numerous bold and funny _Augmented Reality_ projects out there. [ARKit](https://developer.apple.com/arkit/) integrates with `Metal` easily. Finally, the `most influencing` trophy goes to __VR__. It is because of _Virtual Reality_ that we are now able to achieve lower latency, enhanced framerates, as well as more powerful internal and now also [external GPUs](https://developer.apple.com/development-kit/external-graphics/). 

![alt text](https://github.com/MetalKit/images/blob/master/vr.png?raw=true &quot;VR&quot;)

New features were also added to the `Model I/O`, `SpriteKit` and `SceneKit` frameworks. Other interesting additions are the `CoreML` and `Vision` frameworks used for [machine learning](https://developer.apple.com/machine-learning/). This article is only focusing on what's new in `Metal`:

1). __MPS__ - the _Metal Performance Shaders_ are now also available on `macOS` and the new additions to `MPS` include:

* four new image processing primitives (`Image Keypoints`, `Bilinear Rescale`, `Image Statistics`, `Element-wise Arithmetic Operations`).
* new linear algebra objects such as `MPSVector`, `MPSMatrix` and `MPSTemporaryMatrix`, as well as _BLAS-style matrix-matrix and matrix-vector multiplication_ and _LAPACK-style triangular matrix factorization and linear solvers_.
* a dozen new `CNN` primitives.
* the `Binary`, `XNOR`, `Dilated`, `Sub-pixel` and `Transpose` convolutions were added to the already existing `Standard` convolution primitive.
* a new `Neural Network Graph` API was added which is useful for describing neural networks using filter and image nodes.
* the `Recurrent Neural Networks` are now coming to help the `CNNs` one-to-one limitation and implement one-to-many and many-to-many relationships.
        
2). __Argument Buffers__ - likely the most important addition to the framework this year. In the traditional argument model, for each object we would call the various functions to set buffers, textures, samplers linearly and then at the end we would have our draw call for that object.

![alt text](https://github.com/MetalKit/images/blob/master/ArgumentBuffers1.png?raw=true &quot;Argument Buffers 1&quot;)

As you can imagine, the number of calls will increase drastically when multiplying the number of calls with the total number of objects and with the number of frames where all these objects need to be drawn. As a consequence this will limit the number of objects that will appear on the screen eventually. 

![alt text](https://github.com/MetalKit/images/blob/master/ArgumentBuffers2.png?raw=true &quot;Argument Buffers 2&quot;)

`Argument Buffers` introduce an efficient new way of configuring how to use resources by adopting the _indirect_ behavior that the constants have, and applying it to textures, samplers, states, pointers to other buffers, and so on. The argument buffer will now only have `2 API calls per object`: set the argument buffer and then draw. With this approach many more objects can be drawn. 

![alt text](https://github.com/MetalKit/images/blob/master/ArgumentBuffers3.png?raw=true &quot;Argument Buffers 3&quot;)

Using argument buffers is as easy as matching the shader data with the host data:

{% highlight swift %}struct Material {
    float intensity;
    texture2d&lt;float&gt; aTexture;
    sampler aSampler;
}

kernel void compute(constant Material &amp;material [[ buffer(0) ]]) {
    ...
}
{% endhighlight %}

On the `CPU`, the argument buffers are created and used by an __MTLArgumentEncoder__ object and they can be blit between `CPU` and `GPU` easily:

{% highlight swift %}let function = library.makeFunction(name: &quot;compute&quot;)
let encoder = function.makeIndirectArgumentEncoder(bufferIndex: 0)
encoder.setTexture(myTexture, index: 0)
encoder.constantData(at: 1).storeBytes(of: myPosition, as: float4)
{% endhighlight %}

But it can get even better using the `dynamic indexing` feature. A great use case is when rendering crowds. An array of argument buffers can pack the data together for all instances (characters). Then, instead of having two calls per object, now we can have only `2 API calls per frame`: one to set the buffer and one to draw indexed primitives for a large instance count! 

![alt text](https://github.com/MetalKit/images/blob/master/ArgumentBuffers4.png?raw=true &quot;Argument Buffers 4&quot;)

Then the `GPU` will process per-instance geometry and color. The shader will now take an array of argument buffers as input, dynamically pick the character for any instance index, and return the geometry for that object:

{% highlight swift %}vertex Vertex instanced(constant Character *crowd [[ buffer(0) ]],
                        uint id [[instance_id]]) {
    constant Character &amp;instance = crowd[id];
    ...
}
{% endhighlight %}

Another use case for argument buffers is when running particle simulations. For this we have the `resource setting on the GPU` feature which refers to having an array of argument buffers, one buffer for each particle (thread). All the particle properties (position, material, and so on) are created and stored in argument buffers on the `GPU` so when a particle needs a specific property, such as a material, it will copy it from the argument buffers instead of getting it from the `CPU` thus avoiding expensive copies between them. 

![alt text](https://github.com/MetalKit/images/blob/master/ArgumentBuffers5.png?raw=true &quot;Argument Buffers 5&quot;)

A copying kernel is straightforward and lets you assign constant values, do partial or complete copies between a source and a destination object:

{% highlight swift %}kernel void reuse(constant Material &amp;source [[ buffer(0) ]],
                  device Material &amp;destination [[ buffer(1) ]]) {
    destination.intensity = 0.5f;
    destination.aTexture = source.aTexture;
    destination = source;
}
{% endhighlight %}

Finally, we also have the use case of referencing other argument buffers (`multiple indirections`). Imagine a structure to represent an instance (character) that will have a pointer to the `Material` structure such that many instances can point to the same material. Likewise, imagine another structure to represent a tree of nodes where each `Node` would have a pointer to the `Instance` structure which will act as an array of instances in the node:

{% highlight swift %}struct Instance {
    float4 position;
    device Material *material;
}

struct Node {
    device Instance *instances;
}
{% endhighlight %}

&gt; Note: for now, only `Tier 2` devices support all these argument buffer features. Starting with `Metal 2` the `GPU` devices are now classified as either `Tier 1` (integrated) or `Tier 2` (discrete).

3). __Raster Order Groups__ - a new fragment shader synchronization primitive that allows more granular control of the order in which fragment shaders access memory. As an example, when working with custom blending, most graphics `APIs` guarantee that blending happens in draw call order. However, the `GPU` thread parallelism needs a way to prevent race conditions. `Raster Order Groups` do that by providing us with an implicit `Wait` command. 

![alt text](https://github.com/MetalKit/images/blob/master/RasterOrderGroups.png?raw=true &quot;Raster Order Groups&quot;)

In traditional blending mode race conditions are created:

{% highlight swift %}fragment void blend(texture2d&lt;float, access::read_write&gt; out[[ texture(0) ]]) {
    float4 newColor = 0.5f;
    // non-atomic memory access without any synchronization
    float4 oldColor = out.read(position);
    float4 blended = someCustomBlendingFunction(newColor, oldColor);
    out.write(blended, position);
}
{% endhighlight %}

All that is needed is adding the `Raster Order Groups` attribute to the texture (or resource) with conflicting accesses:

{% highlight swift %}fragment void blend(texture2d&lt;float, access::read_write&gt; 
				out[[texture(0), raster_order_group(0)]]) {
    float4 newColor = 0.5f;
    // the GPU now waits on first access to raster ordered memory
    float4 oldColor = out.read(position);
    float4 blended = someCustomBlendingFunction(newColor, oldColor);
    out.write(blended, position);
}
{% endhighlight %}

4). __ProMotion__ - only for iPad Pro displays currently. Without `ProMotion` the typical framerate is `60` FPS (`16.6` ms/frame):

![alt text](https://github.com/MetalKit/images/blob/master/promotion1.png?raw=true &quot;ProMotion 1&quot;)

With `ProMotion` the framerate goes up to `120` FPS (`8.3` ms/frame) which is really useful for user input such as touch gestures or pencil using:

![alt text](https://github.com/MetalKit/images/blob/master/promotion2.png?raw=true &quot;ProMotion 2&quot;)

`ProMotion` also gives us flexibility in when to refresh the display image so we do not need to have a fixed framerate. Without `ProMotion` there is inconsistency in image refreshing which does not bode well for the user experience. Developers usually trade away their peak framerate to constrain all of them to `30` FPS rather than the targeted `48` FPS (`20.83` ms/frame), to achieve consistency:

![alt text](https://github.com/MetalKit/images/blob/master/promotion3.png?raw=true &quot;ProMotion 3&quot;)

With `ProMotion` we now have a refresh point every `4` ms rather than every `16` ms (the vertical white lines):

![alt text](https://github.com/MetalKit/images/blob/master/promotion4.png?raw=true &quot;ProMotion 4&quot;)

`ProMotion` is also helping in cases of dropped frames. Without `ProMotion` we could have a frame that missed the deadline by taking too long to display:

![alt text](https://github.com/MetalKit/images/blob/master/promotion5.png?raw=true &quot;ProMotion 5&quot;)

`ProMotion` fixes this too by only extending the frame with only `4` more ms instead of a whole frame (`16.6` ms):

![alt text](https://github.com/MetalKit/images/blob/master/promotion6.png?raw=true &quot;ProMotion 6&quot;)

`UIKit` animations use `ProMotion` automatically but to use `ProMotion` with `Metal` views you need to opt in by disabling the minimum frame duration in the project’s `Info.plist` file. Then you can use one of the __3__ presentation `APIs`. The traditional __present(drawable:)__ will present the image immediately after the `GPU` has finished rendering the frame (`16.6` ms on fixed framerate displays and `4` ms on `ProMotion` displays). The second `API` is __present(drawable, afterMinimumDuration:)__ and provides maximum consistency from frame to frame on fixed framerate displays. The third `API` is __present(drawable, atTime:)__ and is useful when building custom animation loops or when trying to sync the display image with other outputs such as audio. Here is an example of how to implement it:

{% highlight swift %}let targetTime = 0.1
let drawable = metalLayer.nextDrawable()
commandBuffer.present(drawable, atTime: targetTime)
// after 1-2 frames
let presentationDelay = drawable.presentedTime - targetTime
{% endhighlight %}

First, set a time when you want to display the drawable, then render the scene into a command buffer, then wait for the next frame(s) and finally examine the delay so you can adjust the next frame time.

5). __Direct to Display__ - is the new way to send content from the renderer directly to external displays (eg. head mounted devices used in `VR`) with the least amount of latency. There are two paths an image takes after the `GPU` finished rendering it and before it ends on the display. The first one is the typical `UI` scenario when the system is compositing it with other views and layers for a final image:

￼![alt text](https://github.com/MetalKit/images/blob/master/DirectToDisplay1.png?raw=true &quot;Direct To Display 1&quot;)

When building a full screen application that does not require blending, scaling or other views/layers, the second path is allowing the display direct access to the memory where we rendered to, thus saving a lot of system resources and avoiding a lot of overhead:

￼￼![alt text](https://github.com/MetalKit/images/blob/master/DirectToDisplay2.png?raw=true &quot;Direct To Display 2&quot;)

However, this only happens when certain conditions are met:

* the layer is opaque
* there is no masking or rounded corners
* full screen, or with opaque black bars and background
* the rendered size is at most as large as the display size
* color space and pixel format is compatible with display

The colorspace requirements makes it easier to know when `Direct to Display` mode will work. For example, it is easy to detect if you are using a `P3` display and disable the `P3` mode when trying to use the `Direct to Display` mode.

6). __Other Features__ - include but are not limited to:

* __memory usage queries__ - there are now new `APIs` to query memory use per allocation, as well as total `GPU` memory allocated by the device:
{% highlight swift %}MTLResource.allocatedSize
MTLHeap.currentAllocatedSize
MTLDevice.currentAllocatedSize
{% endhighlight %}
* __SIMDGroup scoped functions__ - allow data sharing between `SIMD` groups directly in the registers by avoiding load/store operations:
￼

￼![alt text](https://github.com/MetalKit/images/blob/master/SIMDGroup.png?raw=true &quot;SIMD Group&quot;)

* __non-uniform threadgroup sizes__ - help us not waste `GPU` cycles and avoid working on edge/bound cases:
￼

￼![alt text](https://github.com/MetalKit/images/blob/master/nonuniform.png?raw=true &quot;Non-uniform Threadgroup Sizes&quot;)

* __Viewport Arrays__ on `macOS` now support up to `16` viewports for the vertex function to choose from when rendering, and is useful for `VR` when combined with instancing.
* __Multisample Pattern Control__ - allows selecting where within a pixel the `MSAA` sample patters are located and it’s useful for custom anti-aliasing.
* __Resource Heaps__ are now also available on `macOS`. It allows controlling the time of memory allocation, fast reallocation, aliasing of resources and group related resources for faster binding.
* other features include:

|Feature|Description|
|:--|:--|
|`Linear Textures`|Create textures from a `MTLBuffer` without copying.|
|`Function Constant for Argument Indexes`|Specialize bytecodes to change the binding index for shader arguments.|
|`Additional Vertex Array Formats`|Add some 1-/2-component vertex formats and a `BGRA8` vertex format.|
|`IOSurface Textures`|Create `MTLTextures` from `IOSurfaces` on `iOS`.|
|`Dual Source Blending`|Additional blending modes with two source parameters.|
||

I made a table with the most important new features, which states whether the feature is new in the latest version of the operating system or not.
￼

￼![alt text](https://github.com/MetalKit/images/blob/master/features.png?raw=true &quot;Feature Table&quot;)

Finally, here are a few lines I wrote to test the differences between my integrated and discrete `GPUs`:
￼

￼![alt text](https://github.com/MetalKit/images/blob/master/gpuCompare.png?raw=true &quot;GPU comparison&quot;)

All images were taken from `WWDC` presentations and the [source code](https://github.com/MetalKit/metal) is posted on `Github` as usual.
 
Until next time!</content><author><name>&lt;a href = &quot;https://twitter.com/gpu3d&quot; target=&quot;_blank&quot;&gt;Marius Horga&lt;/a&gt;</name></author><summary type="html">This year’s WWDC was probably the most important one ever, at least as far as we - the Metal developers - are concerned. I can wholeheartedly say it was the best week of my life, for sure!</summary></entry><entry><title type="html">Working with memory in Metal part 2</title><link href="http://localhost:4000/2017/05/26/working-with-memory-in-metal-part-2.html" rel="alternate" type="text/html" title="Working with memory in Metal part 2" /><published>2017-05-26T00:00:00-05:00</published><updated>2017-05-26T00:00:00-05:00</updated><id>http://localhost:4000/2017/05/26/working-with-memory-in-metal-part-2</id><content type="html" xml:base="http://localhost:4000/2017/05/26/working-with-memory-in-metal-part-2.html">There are a couple of topics we need to discuss in more depth about working with memory. Last time we have seen that to create `MTLBuffer` objects we have 3 options: by creating a new memory allocation with new data, by copying data from an existing allocation into a new allocation and by reusing an existing storage allocation which does not copy data. Since we haven't looked at the memory before, let's check that is actually true. First we copy data into another allocation:
 
{% highlight swift %}let count = 2000
let length = count * MemoryLayout&lt; Float &gt;.stride
var myVector = [Float](repeating: 0, count: count)
let myBuffer = device.makeBuffer(bytes: myVector, length: length, options: [])
withUnsafePointer(to: &amp;myVector) { print($0) }
print(myBuffer.contents())
{% endhighlight %}
 
&gt; Note: the __withUnsafePointer()__ function gives us the memory address of the actual data on the heap instead of the address of the pointer (from the stack) that wraps that data.
 
Your output should look similar to this:
 
{% highlight swift %}0x000000010043e0e0
0x0000000102afd000
{% endhighlight %}

The two data buffers are definitely stored at different memory locations. Now let's use the `no-copy` option:
 
{% highlight swift %}var memory: UnsafeMutableRawPointer? = nil
let alignment = 0x1000
let allocationSize = (length + alignment - 1) &amp; (~(alignment - 1))
posix_memalign(&amp;memory, alignment, allocationSize)
let myBuffer = device.makeBuffer(bytesNoCopy: memory!, 
				 length: allocationSize, 
				 options: [], 
				 deallocator: { (pointer: UnsafeMutableRawPointer, _: Int) in 
					free(pointer) 
				 })
print(memory!)
print(myBuffer.contents())
{% endhighlight %}
 
First, we create a pointer to our data which (data) we'll store on the heap. For this we need to page-align it. We set the page size to be `4K` (`1000` in hexadecimal). We need to also round the buffer size to match the alignment. We used a bitwise `AND` to avoid division which is a very expensive operation. Otherwise we would just round like this: 
 
{% highlight swift %}let allocationSize = ((length + alignment - 1) / alignment) * alignment
{% endhighlight %}
 
Your output should look similar to this:
 
{% highlight swift %}0x000000010300c000
0x000000010300c000
{% endhighlight %}
 
Notice the last three digits in the addresses above? Those come from page-alinging the data because an address is determined by `0 mod pageSize`, hence the last three `0`'s, which makes sense since our page size is `0x1000`.
 
Let's now move to `Storage Modes` which we briefly mentioned last time. There are basically only four main rules to keep in mind, one for each of the storage modes:
 
|Mode|Description|
|:--|:--|
|`Shared`|_default_ on `macOS` buffers, `iOS/tvOS` resources; not available on `macOS` textures.|
|`Private`|mostly use when data is only accessed by `GPU`.|
|`Memoryless`|only for `iOS/tvOS` on-chip temporary render targets (textures).|
|`Managed`|_default_ mode for `macOS` textures; not available on `iOS/tvOS` resources.|
||
 
For a better big picture, here is the full cheat sheet in case you might find it easier to use than remembering the rules above:
 
![alt text](https://github.com/MetalKit/images/blob/master/storage-modes.png?raw=true &quot;Storage Modes&quot;)
 
The most complicated case is when working with `macOS` buffers and when the data needs to be accessed by both the `CPU` and the `GPU`. We choose the storage mode based on whether one or more of the following conditions are true:
 
* __Private__ - for large-sized data that changes at most once, so it is not &quot;dirty&quot; at all. create a source buffer with a `Shared` mode and then blit its data into a destination buffer with a `Private` mode. resource coherency is not necessary in this case as the data is only accessed by the `GPU`. this operation is the least expensive (a one-time cost).
* __Managed__ - for medium-sized data that changes infrequently (every few frames), so it is partially &quot;dirty&quot;. one copy of the data is stored in system memory for the `CPU` and another copy is stored in `GPU` memory. resource coherency is explicitly managed by synchronizing the two copies.
* __Shared__ - for small-sized data that is updated every frame, so it is fully dirty. data resides in the system memory and is visible and modifyable by both the `CPU` and the `GPU`. resource coherency is only guaranteed within command buffer boundaries.
 
How to make sure coherency is guaranteed? First, make sure that all the modifications done by the `CPU` finished before the command buffer was committed (check if the command buffer status property is __MTLCommandBufferStatusCommitted__). After the `GPU` finishes executing the command buffer, the `CPU` should only start making modifications again only after the `GPU` is signaling the `CPU` that the command buffer finished executing (check if the command buffer status property is __MTLCommandBufferStatusCompleted__).
 
Finally, let's see how synchronization is done for `macOS` resources. For buffers: after a `CPU` write use __didModifyRange()__ to inform the `GPU` of the changes so `Metal` can update that data region only; after a `GPU` write use __synchronize(resource:)__ within a blit operation, to refresh the caches so the `CPU` can access the updated data. For textures: after a `CPU` write use one of the two __replace()__ region functions to inform the `GPU` of the changes so `Metal` can update that data region only; after a `GPU` write use one of the two __synchronize()__ functions within a blit operation to allow `Metal` to update the system memory copy after the `GPU` finished modifying the data. 
 
The [source code](https://github.com/MetalKit/metal) is posted on `Github` as usual.
 
Until next time!</content><author><name>&lt;a href = &quot;https://twitter.com/gpu3d&quot; target=&quot;_blank&quot;&gt;Marius Horga&lt;/a&gt;</name></author><summary type="html">There are a couple of topics we need to discuss in more depth about working with memory. Last time we have seen that to create MTLBuffer objects we have 3 options: by creating a new memory allocation with new data, by copying data from an existing allocation into a new allocation and by reusing an existing storage allocation which does not copy data. Since we haven’t looked at the memory before, let’s check that is actually true. First we copy data into another allocation:

let count = 2000
let length = count * MemoryLayout&amp;lt; Float &amp;gt;.stride
var myVector = [Float](repeating: 0, count: count)
let myBuffer = device.makeBuffer(bytes: myVector, length: length, options: [])
withUnsafePointer(to: &amp;amp;myVector) { print($0) }
print(myBuffer.contents())


  Note: the withUnsafePointer() function gives us the memory address of the actual data on the heap instead of the address of the pointer (from the stack) that wraps that data.


Your output should look similar to this:

0x000000010043e0e0
0x0000000102afd000</summary></entry><entry><title type="html">Working with memory in Metal</title><link href="http://localhost:4000/2017/04/30/working-with-memory-in-metal.html" rel="alternate" type="text/html" title="Working with memory in Metal" /><published>2017-04-30T00:00:00-05:00</published><updated>2017-04-30T00:00:00-05:00</updated><id>http://localhost:4000/2017/04/30/working-with-memory-in-metal</id><content type="html" xml:base="http://localhost:4000/2017/04/30/working-with-memory-in-metal.html">Today we look at how memory is managed when working with the `GPU`. The `Metal` framework defines memory sources as `MTLBuffer` objects which are typeless and unformatted allocations of memory (any type of data), and `MTLTexture` objects which are formatted allocations of memory holding image data. We only look at buffers in this article.

To create `MTLBuffer` objects we have 3 options:

* __makeBuffer(length:options:)__ creates a `MTLBuffer` object with a new allocation.
* __makeBuffer(bytes:length:options:)__ copies data from an existing allocation into a new allocation.
* __makeBuffer(bytesNoCopy:length:options:deallocator:)__ reuses an existing storage allocation.

Let's create a couple of buffers and see how data is being sent to the `GPU` and then sent back to the `CPU`. We first create a buffer for both input and output data and initialize them to some values:

{% highlight swift %}let count = 1500
var myVector = [Float](repeating: 0, count: count)
var length = count * MemoryLayout&lt; Float &gt;.stride
var outBuffer = device.makeBuffer(bytes: myVector, length: length, options: [])
for (index, value) in myVector.enumerated() { myVector[index] = Float(index) }
var inBuffer = device.makeBuffer(bytes: myVector, length: length, options: [])
{% endhighlight %}

The new __MemoryLayout&lt; Type &gt;.stride__ syntax was introduced in `Swift 3` to replace the old `strideof(Type)` function. By the way, we use `.stride` instead of `.size` for memory alignment reasons. The __stride__ is the number of bytes moved when a pointer is incremented. The next step is to tell the command encoder about our buffers:

{% highlight swift %}encoder.setBuffer(inBuffer, offset: 0, at: 0)
encoder.setBuffer(outBuffer, offset: 0, at: 1)
{% endhighlight %}

&gt; Note: the Metal Best Practices Guide states that we should always avoid creating buffers when our data is less than __4 KB__ (up to a thousand `Floats`, for example). In this case we should simply use the __setBytes()__ function instead of creating a buffer. 

The final step is to read the data the `GPU` sent back by using the __contents()__ function to bind the memory data to our output buffer:

{% highlight swift %}let result = outBuffer.contents().bindMemory(to: Float.self, capacity: count)
var data = [Float](repeating:0, count: count)
for i in 0 ..&lt; count { data[i] = result[i] }
{% endhighlight %}

`Metal` resources must be configured for fast memory access and driver performance optimizations. Resource __storage modes__ let us define the storage location and access permissions for our buffers and textures. If you take a look above where we created our buffers, we used the default option (__[]__) as the storage mode. 

All `iOS` and `tvOS` devices support a _unified memory model_ where both the `CPU` and the `GPU` share the system memory, while `macOS` devices support a _discrete memory model_ where the `GPU` has its own memory. In `iOS` and `tvOS`, the __Shared__ mode (`MTLStorageModeShared`) defines system memory accessible to both `CPU` and `GPU`, while __Private__ mode (`MTLStorageModePrivate`) defines system memory accessible only to the GPU. The `Shared` mode is the default storage mode on all three operating systems.

![alt text](https://developer.apple.com/library/content/documentation/3DDrawing/Conceptual/MTLBestPracticesGuide/Art/ResourceManagement_iOStvOSMemory_2x.png &quot;iOS and tvOS&quot;)

Besides these two storage modes, `macOS` also has a __Managed__ mode (`MTLStorageModeManaged`) that defines a synchronized memory pair for a resource, with one copy in system memory and another in video memory for faster CPU and GPU local accesses.
 
![alt text](https://developer.apple.com/library/content/documentation/3DDrawing/Conceptual/MTLBestPracticesGuide/Art/ResourceManagement_OSXMemory_2x.png &quot;macOS&quot;)

Now let's look at what happens on the `GPU` when we send it data buffers. Here is a typical vertex shader example:

{% highlight swift %}vertex Vertices vertex_func(const device Vertices *vertices [[buffer(0)]], 
            		    constant Uniforms &amp;uniforms [[buffer(1)]], 
            		    uint vid [[vertex_id]]) 
{
	...
}
{% endhighlight %}

The `Metal Shading Language` implements address space qualifiers to specify the region of memory where a function variable or argument is allocated: 

* __device__ - refers to buffer memory objects allocated from the device memory pool that are both readable and writeable unless the keyword __const__ preceeds it in which case the objects are only readable.
* __constant__ - refers to buffer memory objects allocated from the device memory pool but that are `read-only`. Variables in program scope must be declared in the constant address space and initialized during the declaration statement. The constant address space is optimized for multiple instances executing a graphics or kernel function accessing the same location in the buffer.
* __threadgroup__ - is used to allocate variables used by a kernel functions only and they are allocated for each threadgroup executing the kernel, are shared by all threads in a threadgroup and exist only for the lifetime of the threadgroup that is executing the kernel. 
* __thread__ - refers to the per-thread memory address space. Variables allocated in this address space are not visible to other threads. Variables declared inside a graphics or kernel function are allocated in the thread address space.

As a bonus, let's also look at another way of accessing memory locations in `Swift 3`. This code snippet belongs to a previous article, [The Model I/O framework](http://metalkit.org/2016/08/30/the-model-i-o-framework.html), so we will not go again into details about voxels. Just think of an array that we need to iterate over to get values:

{% highlight swift %}let url = Bundle.main.url(forResource: &quot;teapot&quot;, withExtension: &quot;obj&quot;)
let asset = MDLAsset(url: url)
let voxelArray = MDLVoxelArray(asset: asset, divisions: 10, patchRadius: 0)
if let data = voxelArray.voxelIndices() {
    data.withUnsafeBytes { (voxels: UnsafePointer&lt;MDLVoxelIndex&gt;) -&gt; Void in
        let count = data.count / MemoryLayout&lt;MDLVoxelIndex&gt;.size
        let position = voxelArray.spatialLocation(ofIndex: voxels.pointee)
        print(position)
    }
}
{% endhighlight %} 

In this case, the `MDLVoxelArray` object has a function named `spatialLocation()` which lets us iterate through the array by using an `UnsafePointer` of the `MDLVoxelIndex` type and accessing the data through the `pointee` at each location. In this example we are only printing out the first value found at that address but a simple loop will let us get all of them like this:

{% highlight swift %}var voxelIndex = voxels
for _ in 0..&lt;count {
    let position = voxelArray.spatialLocation(ofIndex: voxelIndex.pointee)
    print(position)
    voxelIndex = voxelIndex.successor()
}
{% endhighlight %}

The [source code](https://github.com/MetalKit/metal) is posted on `Github` as usual.

Until next time!</content><author><name>&lt;a href = &quot;https://twitter.com/gpu3d&quot; target=&quot;_blank&quot;&gt;Marius Horga&lt;/a&gt;</name></author><summary type="html">Today we look at how memory is managed when working with the GPU. The Metal framework defines memory sources as MTLBuffer objects which are typeless and unformatted allocations of memory (any type of data), and MTLTexture objects which are formatted allocations of memory holding image data. We only look at buffers in this article.</summary></entry><entry><title type="html">Ambient Occlusion in Metal</title><link href="http://localhost:4000/2017/03/22/ambient-occlusion-in-metal.html" rel="alternate" type="text/html" title="Ambient Occlusion in Metal" /><published>2017-03-22T00:00:00-05:00</published><updated>2017-03-22T00:00:00-05:00</updated><id>http://localhost:4000/2017/03/22/ambient-occlusion-in-metal</id><content type="html" xml:base="http://localhost:4000/2017/03/22/ambient-occlusion-in-metal.html">Today we will be looking into __ambient occlusion__. We are going to work on the playground we used in [Shadows in Metal part 2](http://metalkit.org/2017/02/28/shadows-in-metal-part-2.html) and build up on that. First, let’s add a new object type - a rectangular box:

{% highlight swift %}struct Box {
    float3 center;
    float size;
    Box(float3 c, float s) {
        center = c;
        size = s;
    }
};
{% endhighlight %}

Next, let’s also add a new distance function for our new struct:

{% highlight swift %}float distToBox(Ray r, Box b) {
    float3 d = abs(r.origin - b.center) - float3(b.size);
    return min(max(d.x, max(d.y, d.z)), 0.0) + length(max(d, 0.0));
}
{% endhighlight %}

Then, update our scene to something new: 

{% highlight swift %}float distToScene(Ray r) {
    Plane p = Plane(0.0);
    float d2p = distToPlane(r, p);
    Sphere s1 = Sphere(float3(0.0, 0.5, 0.0), 8.0);
    Sphere s2 = Sphere(float3(0.0, 0.5, 0.0), 6.0);
    Sphere s3 = Sphere(float3(10., -5., -10.), 15.0);
    Box b = Box(float3(1., 1., -4.), 1.);
    float dtb = distToBox(r, b);
    float d2s1 = distToSphere(r, s1);
    float d2s2 = distToSphere(r, s2);
    float d2s3 = distToSphere(r, s3);
    float dist = differenceOp(d2s1, d2s2);
    dist = differenceOp(dist, d2s3);
    dist = unionOp(dist, dtb);
    dist = unionOp(d2p, dist);
    return dist;
}
{% endhighlight %}

What we did here was to first draw a sphere with a radius of `8`, one with a radius of `6` and take the difference between them. Since they have the same center the smaller one would not be visible unless we made a cross sectioning somehow. That was exactly why we used a third sphere, much larger and with a different center. We took the difference again and we could now see the result of the first difference. Finally, we added a box in there for a nicer, more diverse view. If you run the playground now, you should see something similar: 

![alt text](https://github.com/MetalKit/images/raw/master/ao_1.png &quot;1&quot;)

Next, let’s delete the __lighting()__ and __shadow()__ functions as we don’t need them anymore. Also, delete the __Light__ struct and its two instances inside the kernel. Now let's create an `ambient occlusion` surrogate function:

{% highlight swift %}float ao(float3 pos, float3 n) {
    return n.y * 0.5 + 0.5;
}
{% endhighlight %}

We’re just using the normal’s `y` component for light, which is like having a light directly above. Inside the kernel, right after creating the normal (inside the `else` block), call the `ao()` function:

{% highlight swift %}float o = ao(ray.origin, n);
col = col * o;
{% endhighlight %}

There are no shadows anymore, only a basic (directly above) light. If you run the playground now, you should see something similar:

![alt text](https://github.com/MetalKit/images/raw/master/ao_2.png &quot;2&quot;)

Time to get some real `ambient occlusion` now. _Ambient_ means the light does not come from a well defined light source but rather means general background lighting. _Occlusion_ means how much ambient light is blocked. We take the point on the surface where our ray hits and look at what’s around it. If there’s an object anywhere around it, that will block most of the light in the scene, so this is a dark area. If there’s nothing around it, then the area is well lit. For in between situations though, we need to figure out more precisely how much light was occluded. Introducing the __cone tracing__ concept.

The idea of `cone tracing` is using a cone in the scene, instead of a ray. If the cone intersects an object, we don’t just have a simple `true/false` result. We can find out how much of the cone the object covers at that point. But how do we even trace a cone? We could make a cone using many spheres. Try to imagine several spheres along a line, very small at one end, big at the other end. This is as good a cone approximation we can get here. Here are the steps we want to take:

- Start at the point on the surface
- March out from the surface, along the normal
- For each iteration, determine how much of the sphere is filled by the scene using distance function
- For each iteration, double the distance from the surface, and also double the size of the sphere

Since we are doubling the sphere size at each step, that means we travel out from the surface very fast so we need fewer iterations. That also gives us a nice wide cone. Here is the complete `ao()` function:

{% highlight swift %}float ao(float3 pos, float3 n) {
    float eps = 0.01;
    pos += n * eps * 2.0;
    float occlusion = 0.0;
    for (float i=1.0; i&lt;10.0; i++) {
        float d = distToScene(Ray(pos, float3(0)));
        float coneWidth = 2.0 * eps;
        float occlusionAmount = max(coneWidth - d, 0.);
        float occlusionFactor = occlusionAmount / coneWidth;
        occlusionFactor *= 1.0 - (i / 10.0);
        occlusion = max(occlusion, occlusionFactor);
        eps *= 2.0;
        pos += n * eps;
    }
    return max(0.0, 1.0 - occlusion);
}
{% endhighlight %}

Let's go over the code, line by line. First we define the __eps__ variable which is both the cone radius and the distance from the surface. Then, we move away a bit to prevent hitting surface we're moving away from. Next, we define the __occlusion__ variable which is initially nil (scene is all lit). Then, we enter the loop and at each iteration we get the scene distance, double the radius so we know how much of the cone is occluded, make sure we eliminate negative values for the light, get the amount (ratio) of occlusion scaled by the cone width, set a lower impact for more distant occluders (the iteration count gives us this), preserve the highest occlusion value so far, double the __eps__ value and finally move along the normal by that distance. We then return a value that represents how much light reaches this point.  

Now lets have a __camera__ struct. It needs a position. Instead of camera direction we'll just store a __ray__. Finally the __rayDivergence__ gives us a factor of how much the ray spreads.

{% highlight swift %}struct Camera {
    float3 position;
    Ray ray = Ray(float3(0), float3(0));
    float rayDivergence;
    Camera(float3 pos, Ray r, float div) {
        position = pos;
        ray = r;
        rayDivergence = div;
    }
};
{% endhighlight %}

Next, we need to set up the camera. It needs the camera position, a look-at target, the field of view and the view coordinates:

{% highlight swift %}Camera setupCam(float3 pos, float3 target, float fov, float2 uv, int x) {
    uv *= fov;
    float3 cw = normalize(target - pos );
    float3 cp = float3(0.0, 1.0, 0.0);
    float3 cu = normalize(cross(cw, cp));
    float3 cv = normalize(cross(cu, cw));
    Ray ray = Ray(pos, normalize(uv.x * cu + uv.y * cv + 0.5 * cw));
    Camera cam = Camera(pos, ray, fov / float(x));
    return cam;
}
{% endhighlight %}

Now we just need to initialize the camera. We'll have it circling the scene, looking at the center __(0,0,0)__. Add this to the kernel, just after you set up the `uv` variable:
 
{% highlight swift %}float3 camPos = float3(sin(time) * 10., 3., cos(time) * 10.);
Camera cam = setupCam(camPos, float3(0), 1.25, uv, width);
{% endhighlight %}
 
Then delete the __ray__ variable, and replace everywhere it was used in the kernel with __cam.ray__ instead. If you run the playground now, you should see something similar:

![alt text](https://github.com/MetalKit/images/raw/master/ao_3.png &quot;3&quot;)

To see an animated version of this code, use the `Shadertoy` embedded player below. Just hover over it and click the play button to watch it in action:

&lt;iframe width=&quot;740&quot; height=&quot;450&quot; frameborder=&quot;0&quot; src=&quot;https://www.shadertoy.com/embed/4ltSWf&quot; allowfullscreen&gt;&lt;/iframe&gt;&lt;br /&gt;

The [source code](https://github.com/MetalKit/metal) is posted on `Github` as usual. I want to thanks [Chris](https://twitter.com/_psonice) again for his assistance.

Until next time!</content><author><name>&lt;a href = &quot;https://twitter.com/gpu3d&quot; target=&quot;_blank&quot;&gt;Marius Horga&lt;/a&gt;</name></author><summary type="html">Today we will be looking into ambient occlusion. We are going to work on the playground we used in Shadows in Metal part 2 and build up on that. First, let’s add a new object type - a rectangular box:</summary></entry><entry><title type="html">Shadows in Metal part 2</title><link href="http://localhost:4000/2017/02/28/shadows-in-metal-part-2.html" rel="alternate" type="text/html" title="Shadows in Metal part 2" /><published>2017-02-28T00:00:00-06:00</published><updated>2017-02-28T00:00:00-06:00</updated><id>http://localhost:4000/2017/02/28/shadows-in-metal-part-2</id><content type="html" xml:base="http://localhost:4000/2017/02/28/shadows-in-metal-part-2.html">In this second part of the series, we will be looking into __soft shadows__. We are going to work on the playground we used in [Raymarching in Metal](http://metalkit.org/2016/12/30/raymarching-in-metal.html) and build up on that because it was already set up for `3D` objects. Let’s set up a basic scene that has a sphere, a plane, a light and a ray: 

{% highlight swift %}struct Ray {
    float3 origin;
    float3 direction;
    Ray(float3 o, float3 d) {
        origin = o;
        direction = d;
    }
};

struct Sphere {
    float3 center;
    float radius;
    Sphere(float3 c, float r) {
        center = c;
        radius = r;
    }
};

struct Plane {
    float yCoord;
    Plane(float y) {
        yCoord = y;
    }
};

struct Light {
    float3 position;
    Light(float3 pos) {
        position = pos;
    }
};
{% endhighlight %}

Next, we create a few `distance operation` functions that help us determine distances between elements of the scene: 

{% highlight swift %}float unionOp(float d0, float d1) {
    return min(d0, d1);
}

float differenceOp(float d0, float d1) {
    return max(d0, -d1);
}

float distToSphere(Ray ray, Sphere s) {
    return length(ray.origin - s.center) - s.radius;
}

float distToPlane(Ray ray, Plane plane) {
    return ray.origin.y - plane.yCoord;
}
{% endhighlight %}

Next, we create a __distanceToScene()__ function which gives us the closest distance to any object in the scene. We use these functions to generate a shape that looks like a hollow sphere with holes:

{% highlight swift %}float distToScene(Ray r) {
    Plane p = Plane(0.0);
    float d2p = distToPlane(r, p);
    Sphere s1 = Sphere(float3(2.0), 1.9);
    Sphere s2 = Sphere(float3(0.0, 4.0, 0.0), 4.0);
    Sphere s3 = Sphere(float3(0.0, 4.0, 0.0), 3.9);
    Ray repeatRay = r;
    repeatRay.origin = fract(r.origin / 4.0) * 4.0;
    float d2s1 = distToSphere(repeatRay, s1);
    float d2s2 = distToSphere(r, s2);
    float d2s3 = distToSphere(r, s3);
    float dist = differenceOp(d2s2, d2s3);
    dist = differenceOp(dist, d2s1);
    dist = unionOp(d2p, dist);
    return dist;
}
{% endhighlight %}

Everything we wrote so far is old code, just refactored from the _Raymarching_ article. Let's talk about __normals__ and why they are needed. If we have a flat floor - like our plane - the normal is always `(0, 1, 0)`, that is, pointing up. This case is trivial though. The normal in `3D` space is a `float3` and we need to know its position on the ray. Assume the ray just touches the left side of the sphere. The normal should be `(-1, 0, 0)`, that is, pointing to the left and away from the sphere. If the ray moves slightly to the right of that point, it’s inside the sphere `(eg. -0.001)`. If the ray moves slightly to the left, it’s outside the sphere `(eg. 0.001)`. If we subtract left from right we get `-0.001 - 0.001 = -0.002` which points to the left, so this is our `x` coordinate of the normal. We then repeat this for `y` and `z`. We use a `2D` vector named __eps__ so we can easily do [vector swizzling](https://en.wikipedia.org/wiki/Swizzling_(computer_graphics)) using the chosen value `0.001` for various coordinates as needed in each case: 

{% highlight swift %}float3 getNormal(Ray ray) {
    float2 eps = float2(0.001, 0.0);
    float3 n = float3(distToScene(Ray(ray.origin + eps.xyy, ray.direction)) -
                      distToScene(Ray(ray.origin - eps.xyy, ray.direction)),
                      distToScene(Ray(ray.origin + eps.yxy, ray.direction)) -
                      distToScene(Ray(ray.origin - eps.yxy, ray.direction)),
                      distToScene(Ray(ray.origin + eps.yyx, ray.direction)) -
                      distToScene(Ray(ray.origin - eps.yyx, ray.direction)));
    return normalize(n);
}
{% endhighlight %}

Finally, we are ready to see some visuals. We again use the old `Raymarching` code and at the end of the kernel function we just add the normal so we can interpolate it with the color for every pixel:

{% highlight swift %}kernel void compute(texture2d&lt;float, access::write&gt; output [[texture(0)]],
                    constant float &amp;time [[buffer(0)]],
                    uint2 gid [[thread_position_in_grid]]) {
    int width = output.get_width();
    int height = output.get_height();
    float2 uv = float2(gid) / float2(width, height);
    uv = uv * 2.0 - 1.0;
    uv.y = -uv.y;
    Ray ray = Ray(float3(0., 4., -12), normalize(float3(uv, 1.)));
    float3 col = float3(0.0);
    for (int i=0; i&lt;100; i++) {
        float dist = distToScene(ray);
        if (dist &lt; 0.001) {
            col = float3(1.0);
            break;
        }
        ray.origin += ray.direction * dist;
    }
    float3 n = getNormal(ray);
    output.write(float4(col * n, 1.0), gid);
}
{% endhighlight %}

If you run the playground now you should see a similar image:

![alt text](https://github.com/MetalKit/images/raw/master/shadows_4.png &quot;4&quot;)

Now that we have normals, we can calculate lighting for each pixel in the scene, using the __lighting()__ function. First we need to know the direction to the light (`lightRay`) which we get by normalizing the light position and the current ray. For __diffuse__ lighting we need the angle between the normal and the `lightRay`, that is, the dot product of the two. For __specular__ lighting we need reflections on surfaces, and they depend on the angle we’re looking at. The difference is in this case we first cast a ray into the scene, reflect it from the surface and then we measure the angle between the reflected ray and the `lightRay`. We then take a high power of that value to make it much sharper. Finally we return the combined light:

{% highlight swift %}float lighting(Ray ray, float3 normal, Light light) {
    float3 lightRay = normalize(light.position - ray.origin);
    float diffuse = max(0.0, dot(normal, lightRay));
    float3 reflectedRay = reflect(ray.direction, normal);
    float specular = max(0.0, dot(reflectedRay, lightRay));
    specular = pow(specular, 200.0);
    return diffuse + specular;
}
{% endhighlight %}

Replace the last line in the kernel function with these lines:

{% highlight swift %}Light light = Light(float3(sin(time) * 10.0, 5.0, cos(time) * 10.0));
float l = lighting(ray, n, light);
output.write(float4(col * l, 1.0), gid);
{% endhighlight %}

If you run the playground now you should see a similar image:

![alt text](https://github.com/MetalKit/images/raw/master/shadows_5.png &quot;5&quot;)

Next, shadows! We pretty much use the __shadow()__ function from the first part of this series, with few modifications. We normalize the direction of the light (`lightDir`) and then we just keep updating `distAlongRay` as we march along the ray:

{% highlight swift %}float shadow(Ray ray, Light light) {
    float3 lightDir = light.position - ray.origin;
    float lightDist = length(lightDir);
    lightDir = normalize(lightDir);
    float distAlongRay = 0.01;
    for (int i=0; i&lt;100; i++) {
        Ray lightRay = Ray(ray.origin + lightDir * distAlongRay, lightDir);
        float dist = distToScene(lightRay);
        if (dist &lt; 0.001) {
            return 0.0;
            break;
        }
        distAlongRay += dist;
        if (distAlongRay &gt; lightDist) { break; }
    }
    return 1.0;
}
{% endhighlight %}

Replace the last line in the kernel function with these lines:

{% highlight swift %}float s = shadow(ray, light);
output.write(float4(col * l * s, 1.0), gid);
{% endhighlight %}

If you run the playground now you should see a similar image:

![alt text](https://github.com/MetalKit/images/raw/master/shadows_6.png &quot;6&quot;)

Let's get some `soft shadows` in the scene. In real life, a shadow spreads out the farther it gets from an object. For example, if there is a cube on the floor, at a cube's vertex we get a sharp shadow but farther away from the cube it looks more like a blurred shadow. In other words, we start at some point on the floor, we march towards the light and either hit or miss. Hard shadows are straightforward: we hit something, it's in the shadow. Soft shadows have in-between stages. Update the __shadow()__ function with these lines:

{% highlight swift %}float shadow(Ray ray, float k, Light l) {
    float3 lightDir = l.position - ray.origin;
    float lightDist = length(lightDir);
    lightDir = normalize(lightDir);
    float eps = 0.1;
    float distAlongRay = eps * 2.0;
    float light = 1.0;
    for (int i=0; i&lt;100; i++) {
        Ray lightRay = Ray(ray.origin + lightDir * distAlongRay, lightDir);
        float dist = distToScene(lightRay);
        light = min(light, 1.0 - (eps - dist) / eps);
        distAlongRay += dist * 0.5;
        eps += dist * k;
        if (distAlongRay &gt; lightDist) { break; }
    }
    return max(light, 0.0);
}
{% endhighlight %}

You will notice that we are starting with a white (`1.0`) light this time and we use an attenuator (__k__) to get various (intermediate) values of light. The __eps__ variable tells us how much wider the beam is as we go out into the scene. A thin beam means sharp shadow while a wide beam means soft shadow. We start with a small `distAlongRay` because otherwise the surface at this point would shadow itself. We then travel along the ray as we did for the hard shadows, then we get the distance to the scene, after that we subtract `dist` from `eps` (the beam width) and divide it by `eps`. This gives us the percentage of beam covered. If we invert it (`1 - beam width`) we get the percentage of beam that is in the light. We take the minimum of this new value and `light` to preserve the darkest shadow as we march along the ray. We then again move along the ray and increase the beam width in proportion to the distance traveled and scaled by `k`. If we're past the light, we break out of the loop. Finally, we want to avoid negative values for the light so we return the maximum between __0.0__ and the value of light. Now let's adapt the kernel code to work with the new `shadow()` function:

{% highlight swift %}float3 col = float3(1.0);
bool hit = false;
for (int i=0; i&lt;200; i++) {
    float dist = distToScene(ray);
    if (dist &lt; 0.001) {
        hit = true;
        break;
    }
    ray.origin += ray.direction * dist;
}
if (!hit) {
    col = float3(0.5);
} else {
    float3 n = getNormal(ray);
    Light light = Light(float3(sin(time) * 10.0, 5.0, cos(time) * 10.0));
    float l = lighting(ray, n, light);
    float s = shadow(ray, 0.3, light);
    col = col * l * s;
}
Light light2 = Light(float3(0.0, 5.0, -15.0));
float3 lightRay = normalize(light2.position - ray.origin);
float fl = max(0.0, dot(getNormal(ray), lightRay) / 2.0);
col = col + fl;
output.write(float4(col, 1.0), gid);
{% endhighlight %}

Notice we switched to having a rather white color by default. Then we added a boolean named __hit__ that tells us if we hit the object or not. We determine we have a hit if the distance to scene is within __0.001__. If we didn't hit anything, just color everything in grey, otherwise determine the shadow value. At the end we just add another (fixed) light source in front of the scene so see the shadows in greater detail. If you run the playground now you should see a similar image:

![alt text](https://github.com/MetalKit/images/raw/master/shadows_7.png &quot;7&quot;)

To see an animated version of this code, use the `Shadertoy` embedded player below. Just hover over it and click the play button to watch it in action:

&lt;iframe width=&quot;740&quot; height=&quot;450&quot; frameborder=&quot;0&quot; src=&quot;https://www.shadertoy.com/embed/XltSWf&quot; allowfullscreen&gt;&lt;/iframe&gt;&lt;br /&gt;

The [source code](https://github.com/MetalKit/metal) is posted on `Github` as usual.

Until next time!</content><author><name>by &lt;a href = &quot;https://twitter.com/MTLDevice&quot; target=&quot;_blank&quot;&gt;Marius Horga&lt;/a&gt;</name></author><summary type="html">In this second part of the series, we will be looking into soft shadows. We are going to work on the playground we used in Raymarching in Metal and build up on that because it was already set up for 3D objects. Let’s set up a basic scene that has a sphere, a plane, a light and a ray:</summary></entry></feed>
